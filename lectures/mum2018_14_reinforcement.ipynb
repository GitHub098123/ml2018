{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<big><big><big><big><big><big>Metody uczenia maszynowego</big></big></big></big></big></big>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<big><big><big><big><big>Reinforcement learning</big></big></big></big></big>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<id=tocheading><big><big><big><big>Spis treści</big></big></big></big>\n",
    "<div id=\"toc\"></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "\n",
    "from bokeh.io import gridplot, output_file, show\n",
    "from bokeh.plotting import figure, output_notebook\n",
    "from bkcharts import Scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"389b8ff2-bc7f-4af9-8034-9d39b4728e37\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      var el = document.getElementById(\"389b8ff2-bc7f-4af9-8034-9d39b4728e37\");\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"389b8ff2-bc7f-4af9-8034-9d39b4728e37\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '389b8ff2-bc7f-4af9-8034-9d39b4728e37' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.7.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.7.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.7.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.7.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "      document.getElementById(\"389b8ff2-bc7f-4af9-8034-9d39b4728e37\").textContent = \"BokehJS is loading...\";\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.7.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.7.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.7.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.7.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.7.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.7.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"389b8ff2-bc7f-4af9-8034-9d39b4728e37\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_notebook()\n",
    "sns.set(font_scale=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Image inclusion\n",
    "<img src=\"../mum_figures/\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Uczenie nadzorowane, nienadzorowane, ze wzmocnieniem\n",
    "1. __poznawcze__ pod _nadzorem_ nauczyciela podającego prawidłowe odpowiedzi (akcje do podjęcia)\n",
    "  * niepraktyczne w zadaniach interaktywnych\n",
    "2. __behawioralne__ przez interakcję ze __środowiskiem__: uczenie __reinforcement__\n",
    "  * agent usiłuje rozwiązać zadanie\n",
    "  * w nieopisanym środowisku agent musi uczyć się z doświadczenia\n",
    "  * trudne (lub niemożliwe) zdobycie poprawnych odpowiedzi\n",
    "3. podejścia do uczenia __ze wzmocnieniem__\n",
    "  * __klasyczne__ przez serię nagród/kar dla osiągnięcia dobrego zachowania\n",
    "  * __współczesne__ przez programowanie dynamiczne (__planowanie__)\n",
    "4. reinforcement jest różne od nienadzorowanego\n",
    "  * RL stara się maksymalizować __sygnał zwrotu__ (_ang_. __reward__)\n",
    "  * RL nie stara się szukać struktury problemu (chociaż może to być przydatne)\n",
    "5. exploration - exploitation tradeoff\n",
    "  * __exploracja__ przeglądanie przestrzeni rozwiązań w poszukiwaniu nowych\n",
    "  * __eksploatacja__ maksymalne wykorzystanie już znalezionych\n",
    "  * niemożliwe jest skupienie się wyłączniena jednym "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elementy uczenia ze wzmocnieniem\n",
    "1. __polityka__ (_ang_. __policy__) definiuje sposób zachowania agenta w danym momencie\n",
    "  * odwzorowanie ze stanów do akcji do podjęcia w tym stanie\n",
    "  * tablica, albo proces wyszukiwania\n",
    "  * zwykle (często) stochastyczne\n",
    "2. __sygnał wzmocnienia__ (_ang_. __reward__) określa cel\n",
    "  * w każdym kroku środowisko wysyła __pojedynczy__ sygnał wzmocnienia\n",
    "  * agent maksymalizuje __całkowity__ reward\n",
    "  * na podstawie reward agnet moze zmienić swoją politykę\n",
    "  * agent __nie ma__ wpływu na sam sposób generowania sygnału \n",
    "  * reward to (stochastyczna) funkcja stanu i podjętej akcji  \n",
    "3. funkcja __wartości__ (_ang_. __value__) stanu\n",
    "  * określa co jest korzystne dla agenta __na dłuższą__ metę\n",
    "  * wartość stanu, to sumaryczne (wartość oczekiwana) wzmocnienie (reward), które agent może zdobyć w przyszłości __począwszy__ od tego stanu\n",
    "    * jesli baterie odkurzacza są prawie naładowane, to wartość tego stanu jest wyższa\n",
    "  * reward stanu może być niskie (zmęczenie organizmu) ale wartość stanu wysoka (już blisko szczytu)\n",
    "    * i odwrotnie\n",
    "  * wartość przeliczana przez cały okres działania agenta\n",
    "4. model środowiska\n",
    "  * symuluje działanie rzeczywistego środowiska"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przykłady (Sutton)\n",
    "1. gra w szachy i wybór ruchu \n",
    "  * zależny od sytuacji na planszy\n",
    "  * prawdopodobnych odpowiedzi na wiele ruchów wprzód\n",
    "2. kontroler maszyn w czasie rzeczywistym\n",
    "  * zależny od kosztu i zysków\n",
    "  * granicznych warunków\n",
    "3. młody gepard (gazela, robot, ...) wstajacy na nogi zaraz po urodzeniu\n",
    "4. samojezdny odkurzacz\n",
    "  * stan baterii\n",
    "  * stan środowiska\n",
    "  * częściowa wiedza o środowisku\n",
    "5. i wiele innych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gra w kółko i krzyżyk\n",
    "1. minimax nie znajdzie optymalnej startegii\n",
    "  * wybiera akcję, po której, przy __najlepszym__ ruchu przeciwnika, __nie__ przegra\n",
    "2. programowanie dynamiczne wymaga __pełnego__ opisu środowiska (stanów, akcji, prawdopodobieństw)\n",
    "3. reinforcement z funkcją wartości stanów\n",
    "  * wszystkie stany wygrane uzyskują wartość $1$\n",
    "  * przegrywające $0$\n",
    "  * wszystkie inne $0.5$ \n",
    "    * brak innej wiedzy\n",
    "  * RL rozpoczyna grę z przeciwnikiem\n",
    "    * większość ruchów __zachłannych__: do stanu o __wyższej__ wartości; czasem losowe\n",
    "      * eksploracja -- eksploatacja\n",
    "  * stany zmieniają wartość\n",
    "    $$V(s)=V(s)+\\alpha\\left[V(s')-V(s)\\right]$$      * to eksploatacja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem k-jednorękich bandytów\n",
    "1. k-jednorękich bandytów\n",
    "  * k maszyn\n",
    "  * każda ma jakąś __nieznaną__ stopę zwrotu\n",
    "  * akcja: wybór maszyny\n",
    "  * reward: ile maszyna zwróci po pociągnięciu wajchy\n",
    "  * algorytm $\\epsilon$-zachłanny\n",
    "  $$A=\\begin{cases}\n",
    "  \\underset{a}{\\arg\\max}Q(a)&\\text{z prawdopodobieństwem}\\hskip1em 1-\\epsilon\\\\\n",
    "  losowa&\\text{z prawdopodobieństwem}\\hskip1em \\epsilon\n",
    "  \\end{cases}$$\n",
    "    * eksploracja - eksploatacja\n",
    "  * wyniki będą różne w zależności od wartości $\\epsilon$\n",
    "    * $\\epsilon=0$ - zachłanny\n",
    "      * z początku rośnie szybciej, by szybko wyhamować już bez zmian\n",
    "      * wybiera nieoptymalne akcje\n",
    "      * optymalne akcje tylko w ułamku doświadczeń\n",
    "    * $\\epsilon>0$\n",
    "      * tu algorytm wciąż eksploruje\n",
    "      * większe $\\epsilon$ daje większą eksplorację, jednak w końcu zachowuje się gorzej\n",
    "      * mniejsze $\\epsilon$ uczy się wolniej, ale daje w końcu lepsze wyniki\n",
    "        * można zmniejszać $\\epsilon$\n",
    "    * zysk $\\epsilon$-zachłannego zalezy bardzo od zadania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estymacja średniej stopy zwrotu\n",
    "2. estymacja średniej stopy zwrotu\n",
    "  $$\\begin{align}\n",
    "  Q_{n+1}&=\\frac{1}{n}\\sum_{t=1}^nR_t=\\frac{1}{n}\\left[R_n+\\sum_{t=1}^{n-1}R_t\\right]\\\\\n",
    "  &=\\frac{1}{n}\\left[R_n+(n-1)Q_n\\right]=\\frac{1}{n}\\left[R_n+nQ_n-Q_n\\right]\n",
    "  =Q_n+\\frac{1}{n}\\left[R_n-Q_n\\right]\n",
    "  \\end{align}$$\n",
    "  * $\\alpha=\\frac{1}{n}$ dla środowiska __stacjonarnego__ gdy bandyci nie zmieniają się\n",
    "3. Estymacja dla $\\alpha$\n",
    "  $$\\begin{align}\n",
    "  Q_{n+1}&=Q_n+\\alpha\\left[R_n-Q_n\\right]=\\alpha{}R_n+(1-\\alpha)Q_n\\\\\n",
    "  &=\\alpha{}R_n+(1-\\alpha)\\left[\\alpha{}R_{n-1}+(1-\\alpha)Q_{n-1}\\right]\\\\\n",
    "  &=\\dots\\\\\n",
    "  &=\\alpha{}R_n+(1-\\alpha)\\alpha{}R_{n-1}+\\dots+(1-\\alpha)^{n-1}\\alpha{}R_{1}+(1-\\alpha)^nQ_1\\\\\n",
    "  &=(1-\\alpha)^nQ_1+\\sum_{t=1}^n\\alpha(1-\\alpha)^{n-t}R_t\n",
    "  \\end{align}$$\n",
    "  * suma ważona dla $\\alpha\\in(0,1]$\n",
    "  * waga $\\alpha(1-\\alpha)^{n-t}$ zależy od liczby obserwacji wstecz\n",
    "  * jeśli $\\alpha=11$, to $1-\\alpha=0$ i wszystkie istotne wagi idą do ostatniej reward $R_n$\n",
    "    * konwencja: $0^0=1$\n",
    "  \n",
    "  * jeśli __parametr kroku__ $\\alpha$ będzie __odpowiednio__ zmniejszany, to algorytm zbieżny\n",
    "  $$\\sum_{k=1}^\\infty\\alpha_n(a)=\\infty\\hskip1em\\text{oraz}\\hskip1em\\sum_{k=1}^\\infty\\alpha_n^2(a)<\\infty$$\n",
    "    * niespełnienie drugiego warunku może być przydatne w __niestacjonarnym__ (czy niestabilnym) środowisku\n",
    "  * to metoda __temporal difference__\n",
    "4. optymistyczne wartości początkowe\n",
    "  * standardowo $Q_1=0$\n",
    "  * optymistyczna wartość większa od rzeczywistej spowoduje większą eksplorację\n",
    "  * praktyczne dla stacjonarnych, może zaburzyć dla niestacjonarnych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "### Upper confidence bound\n",
    "1. Wybór akcje według $$A_t=\\underset{a}{\\arg\\max}\\left[Q_t(a)+c\\sqrt{\\frac{\\log\\,t}{N_t(a)}}\\,\\right]$$\n",
    "gdzie dwa składniki \n",
    "  * $Q_t(a)$ to estymacja wartości akcji $a$\n",
    "  * miara niepewności (wariancji) $\\sqrt{\\frac{\\log\\,t}{N_t(a)}}$\n",
    "2. UCB radzi sobie bardzo dobrze w zadaniach typu k-bandytów  \n",
    "  * może być słabiej gdy zadania są niestacjonarne\n",
    "  * czasem problem z radzeniem sobie z całą przestrzenią rozwiązań"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upper Confidence bound applied to Trees UCT (albo MCTS - Monte Carlo Tree Search)\n",
    "  * drzewo gry\n",
    "  * przeciwnicy wybierają ruch na zmianę\n",
    "  * w każdym cyklu rozwijając drzewo\n",
    "    1. gracz wybiera optymalny wierzchołek wydług miary UCB\n",
    "    2. z wybranego węzła (liścia) wykonuje wszystkie możliwe ruchy\n",
    "    3. dla każdego z nich wykonuje __losowe symulacje__ gier zapamiętując wyniki\n",
    "    4. poprawia wartości $Q_t(a)$\n",
    "      * dla liścia uśrednia wyniki wszystkich gier i zapisuje z $N_t(a)=1$\n",
    "      * poprawia wartości $Q_t(A_t),\\;t=t-1,\\dots,0$ po drodze aż do korzenia\n",
    "  * dobrze sprawdza się dla gier\n",
    "  * przeszukuje w drzewie __tylko__ poddrzewa, które dają nadzieje\n",
    "  * jeden z pierwszych algorytmów, który dał w miarę sensowne wyniki w Go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metody gradientowe (!?!?!?!?!?!!!!?)\n",
    "1. Podstawowe metody estymują wartości akcji i na tej podstawie wybierają najlepszą\n",
    "2. można zdefiniować __preferencję__ dla akcji $H_t(a)$\n",
    "  * preferencja __nie ma__ interpretacji w sensie nagrody (reward)\n",
    "  * im wyższa preferencja __względem innych__ tym częściej będzie wybrana\n",
    "  \n",
    "  $$\\pi_t(a)=p(A_t=a)=\\frac{\\exp(H_t(a))}{\\sum_b\\exp(H_t(b))}$$\n",
    "3. Algorytm __stochastycznego wzrostu gradientu__\n",
    "\n",
    "  $$\\begin{align}\n",
    "  H_{t+1}&=H_{t}+\\alpha\\frac{\\partial\\mathbb{E}[R_t]}{\\partial{}H_t(a)}\\\\\n",
    "  \\mathbb{E}[R_t]&=\\sum_b\\pi_t(b)q_\\ast(b)\n",
    "  \\end{align}$$\n",
    "  gdzie $\\mathbb{E}[R+t]$ jest miarą jakości, a $q_\\ast(b)$ jest (nieznaną) __optymalną__ funkcją state-value\n",
    "  * pochodna jakości działania względem __preferencji__\n",
    "4. jaką postać ma gradient? jak policzyć pochodną oczekiwanej efektywności po preferencji $H_t(a)\\dots$?\n",
    "\n",
    "  $$\\begin{align}\n",
    "\\frac{\\partial\\mathbb{E}[R_t]}{\\partial{}H_t(a)}&=\\frac{\\partial}{\\partial{}H_t(a)}\\sum_bq_\\ast\\pi_t(b)=\\sum_bq_\\ast\\frac{\\partial\\pi_t(b)}{\\partial{}H_t(a)}\\\\\n",
    "&=\\sum_bq_\\ast\\frac{\\partial}{\\partial{}H_t(a)}\\frac{\\exp(H_t()b)}{\\sum_c\\exp(H_t(c))}\\\\\n",
    "&\\\\\n",
    "&\\hskip3em\\text{można odjąć wartość skalarną $X_t$ niezależną od $b$}\\\\\n",
    "&\\hskip3em\\text{bo $\\pi_t()$ sumują się do $1$, a gradienty do $0$}\\\\\n",
    "&\\\\\n",
    "&=\\sum_b(q_\\ast-X_t)\\frac{\\partial}{\\partial{}H_t(a)}\\frac{\\exp(H_t(b))}{\\sum_c\\exp(H_t(c))}\\\\\n",
    "&\\\\\n",
    "&\\hskip3em\\text{gradient $\\pi_t(b)$, jako funkcji softmax, ma postać}\\\\\n",
    "&\\hskip5em\\frac{\\partial\\pi_t(b)}{\\partial{}H_t(a)}=\\pi_t(b)[1_{a=b}-\\pi_t(a)]\\\\\n",
    "&\\\\\n",
    "&=\\sum_b(q_\\ast-X_t)\\,\\pi_t(b)\\,[1_{a=b}-\\pi_t(a)]\\\\\n",
    "&=\\mathbb{E}_\\pi\\left[(q_\\ast-X_t)[1_{a=b}-\\pi_t(a)]\\right]\\\\\n",
    "&\\\\\n",
    "&\\hskip3em\\text{ponieważ optymalna action-value $\\mathbb{E}[R_t\\mid A_t]=q_\\ast(A_t)$}\\\\\n",
    "&\\hskip3em\\text{niech $q_\\ast(A_t)=R_t$ ($R_t$ nieskorelowane)}\\\\\n",
    "&\\hskip3em\\text{oraz przyjąć baseline $X_t=Q_{t+1}$ - średniej wartości zwrotu}\\\\\n",
    "&\\\\\n",
    "&=\\mathbb{E}_\\pi\\left[(R_t-Q_{t+1})[1_{a=b}-\\pi_t(a)]\\right]\n",
    "\\end{align}$$\n",
    "\n",
    "  gdzie $Q_{t+1}=1/t\\sum_{k=1}^tR_k$\n",
    "5. W ten sposób mamy __wzory na poprawę preferencji__\n",
    "  * dla wybranej akcji $A_t$ $$H_{t+1}(A_t)=H(A_t)+\\alpha(R_t-Q_{t+1})(1-\\pi_t(A_t))$$\n",
    "  * dla akcji niewybranych $a\\neq A_t$ $$H_{t+1}(a)=H(A_t)-\\alpha(R_t-Q_{t+1})\\pi_t(a)$$\n",
    "  * składnik $Q_{t+1}$ służy jako __odniesienie__ z którym porównywana jest wartość $R_t$ uzyskana\n",
    "    * jeśli $R_t$ jest __większa__, to preferencja będzie __zwiększana__\n",
    "    * preferencje __nie wybranych__ akcji będą zmniejszane __proporcjonalnie__ do szans ich wybrania $\\pi_t(a)$\n",
    "  * ominięcie składnika odniesienia wpływa osłabiająco na algorytm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meęèéêëėētoóôöòõœøōdy (niektóre)\n",
    "(bez określonego porządku)\n",
    "1. metody ewolucyjne\n",
    "  * symulują zachowanie agentów w czasie całego ich życia\n",
    "  * nie określają wartości stanów\n",
    "  * nie kontaktują się ze środowiskiem\n",
    "  * abstrahują od definicji policy, której szukają, jako funkcji stanów w akcje\n",
    "2. przeszukiwanie przestrzeni rozwiązań\n",
    "  * optymalizacja policy jako zbioru numerycznych parametrów i poszukiwanie kierunku poprawy\n",
    "  * __policy gradient__\n",
    "3. przeszukiwanie metodami Monte-Carlo\n",
    "  * określenie lepszych/gorszych obszarów\n",
    "  * optymalizacja dylematu exploration-exploitation\n",
    "  * __upper confidence bound__\n",
    "  * także proste metody losowe i optymalizacji __hill climbing__\n",
    "4. __programowanie dynamiczne__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Law of Effect\n",
    "_Of several responses made to the same situation, those which are accompanied or closely followed by satisfaction to the animal will, other things being equal, be more firmly connected with the situation, so that, when it recurs, they will be more likely to recur; those which are accompanied or closely followed by discomfort to the animal will, other things being equal, have their connections with this situation weakened, so that, when it recurs, they will be less likely to occur. The greater the satisfaction or discomfort, the greater the strengthening or weakening of the bond_ (Edward Thorndike, 1911)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit assignment problem\n",
    "1. __programowanie dynamiczne__\n",
    "  * decyzje podejmowane w krokach\n",
    "  * żądanie niskiego __aktualnego__ kosztu musi być zrównoważone z __wysokim__ kosztem w przyszłości\n",
    "  * kredyt musi być zrównoważony przez wszystkie podejmowane decyzje\n",
    "  \n",
    "<big>___W jaki sposób agent może podnieść swoje długo-terminowe wyniki w stochastycznym środowisku gdy osiągnięcie zysków może się wiązać kosztem wyników krótko-terminowych?___</big>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesy decyzyjne Markowa\n",
    "1. agent działa zgodnie z założeniami __skończenie stanowego procesu Markowa__\n",
    "  <img style=\"float: right;\" src=\"../mum_figures/markov_process.png\" width=\"45%\">\n",
    "  * proces rozwija się __probabilistycznie__\n",
    "  * ma __skończoną__ liczbę stanów\n",
    "  * __nie zawiera__ statystyk poprzedniego zachowania\n",
    "  * dla każdego stanu istnieje __skończona__ liczba akcji możliwych do podjęcia\n",
    "  * każda akcja jest związana z pewnym __kosztem__\n",
    "  * wszystko dzieje się w czasie __dyskretnym__\n",
    "  \n",
    "    __Stan środowiska jest pewnym _podsumowaniem_ całego dotychczasowego doświadczenia agenta z tym środowiskiem. Informacja konieczna dla agenta dla przewidywania przyszłości jest zawarta w tym podsumowaniu__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Akcje\n",
    "2. dla każdego stanu $s$ istnieje __skończony__ zbiór akcji $a_{s,s'}$\n",
    "  <img style=\"float: right;\" src=\"../mum_figures/markov_process.png\" width=\"45%\">\n",
    "  * akcja przeprowadza __probabilistycznie__ stan $S_t=s$ w stan $S_{n+1}=s'$  \n",
    "    1. $p_{s,s'}(a)\\ge0$\n",
    "    2. $\\sum_{s'}p_{s,s'}(a)=1$\n",
    "  ---\n",
    "<big>__Własność Markowa: prawdopodobieństwo przejścia ze stanu $s$ w stan $s'$ zależy _wyłącznie_ od aktualnego stanu $s$ oraz akcji $a_{s,s'}$__</big>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward\n",
    "3. każde przejście jest związane z reward (kosztem) $r(s, a, s')$ dla agenta\n",
    "<img style=\"float: right;\" src=\"../mum_figures/markov_process.png\" width=\"45%\">  \n",
    "  * $r(s, a_{s,s'}, s')$ jest ustaloną funkcją\n",
    "  * szachy: +1 za wygraną, -1 za przegraną, 0 w innym przypadku\n",
    "  * odkurzacz: \n",
    "    * dodatnie wartości za odkurzanie dawno nie odwiedzanego terenu\n",
    "    * dodatnie ale mniejsze za odkurzanie terenu widzanego niedawno\n",
    "    * zerowa za utrzymywanie baterii w stanie naładowanym\n",
    "    * wysoka ujemna za rozładowanie baterii\n",
    "  * reward jest __deklaracją celu__\n",
    "    * __nie jest__ deklaracją sposobu jego osiągnięcia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zwrot (return)\n",
    "  <big>__Celem i zadaniem procesu jest maksymalizacja _oczekiwanej wartości skumulowanej sumy_ uzyskanych wartości skalarnych (reward)__</big>\n",
    "1. Celem jest maksymalizacja oczekiwanej wartości skumulowanych reward\n",
    "  * najprościej $$G_t=R_{t+1}+R_{t+2}+\\dots+R_{T}$$\n",
    "  gdzie $T$ jest numerem ostatniego kroku\n",
    "2. __epizod__ oznacza naturalną skończoną sekwencję zdarzeń agent-środowisko\n",
    "  * pojedyncze gry, codzienne odkurzanie, etc.\n",
    "  * epizod ma stan końcowy (terminalny)\n",
    "    * po końcu następuje __powrót__ do ustalonego (lub określanego inaczej, np. losowanego) stanu początkowego\n",
    "  * zadania w epizodach to, nomen omen, __zdania epizodyczne__\n",
    "3. zadania __ciągłe__\n",
    "  * nie dzielą się na epizody\n",
    "  * wtedy $$G_t=R_{t+1}+\\gamma{}R_{t+2}+\\gamma{}R_{t+3}\\dots=\\sum_{k=0}^\\infty\\gamma^kR_{t+k+1}$$\n",
    "  * $0\\ge\\gamma\\ge1$ jest czynnikiem __discount__\n",
    "  * regulacja $\\gamma$ pozwala sterować na ile agent interesuje się przyszłością\n",
    "    * małe gamma to zainteresowanie celami bliskimi\n",
    "    * $\\gamma=0$ daje agenta __krótkowzrocznego__ (ang. myopic) zainteresowanego __tylko__ chwilą obecną\n",
    "    * dla $\\gamma$ bliższego $1$ ważniejsza staje się przyszłość (cała historia)\n",
    "4. Zunifikowana notacja to $$G_t=\\sum_{k=0}^{T-t-1}\\gamma^kR_{t+k+1}$$ \n",
    "  * w tym $T=\\infty$\n",
    "  * a także $\\gamma=1$ (ale __nie jednocześnie z $T=\\infty$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy (strategia, polityka)\n",
    "4. __policy__ $\\pi$ to __odwzorowanie stanów w akcje__\n",
    "  * deterministyczne $\\pi(s)$ opisuje akcję podejmowaną w stanie s\n",
    "  * stochastyczne $\\pi_t(a\\mid s)=P(A_t=a\\mid S_t=s)$\n",
    "  * policy jest __dopuszczalna__ (ang. admissible) jeśli $\\pi(s)\\in\\mathcal{A}_s$ gdzie $\\mathcal{A}_s$ jest zbiorem wszystkich możliwych akcji w stanie $s$\n",
    "  * policy może być\n",
    "    * __niestacjonarna__ jeśli zmienia się w czasie\n",
    "    * __stacjonarna__ jeśli jest niezależna od kroku\n",
    "5. Proces Markowa może być\n",
    "  * __stacjonarny__, gdy $p_{s,s'}$ nie zmieniają się\n",
    "  * __niestacjonarny__ w przeciwnym wypadku"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funkcje wartości (value)\n",
    "1. funkcja __wartości__ (_ang_. __value__, __state-value__) $v_\\pi(s)$ określa jakość stanu $s$ przy polityce (policy) $\\pi$\n",
    "  * czyli oczekiwana wartość zwrotu w stanie $s$\n",
    "\n",
    "  $$\\begin{align}\n",
    "v_\\pi(s)&=\\mathbb{E}_\\pi\\left[G_t\\mid S_t=s\\right]\\\\\n",
    "&=\\mathbb{E}_\\pi\\left[\\sum_{k=0}^\\infty\\gamma^kR_{t+k+1}\\mid S_t=s\\right]\n",
    "\\end{align}$$\n",
    "2. podobnie wartość podjęcia akcji (_ang_. __action-value__) $a$ w stanie $s$ w polityce $\\pi$ \n",
    "\n",
    "  $$\\begin{align}\n",
    "q_\\pi(s, a)&=\\mathbb{E}_\\pi\\left[G_t\\mid S_t=s, A_t=a\\right]\\\\\n",
    "&=\\mathbb{E}_\\pi\\left[\\sum_{k=0}^\\infty\\gamma^kR_{t+k+1}\\mid S_t=s, A_t=a\\right]\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Równanie Bellmana\n",
    "3. wartości stanu użyte w trakcie uczenia spełniają zależności rekurencyjne\n",
    "\n",
    "  $$\\begin{align}\n",
    "v_\\pi(s)&=\\mathbb{E}_\\pi\\left[G_t\\mid S_t=s\\right]\\\\\n",
    "&=\\mathbb{E}_\\pi\\left[\\sum_{k=0}^\\infty\\gamma^kR_{t+k+1}\\mid S_t=s\\right]\\\\\n",
    "&=\\mathbb{E}_\\pi\\left[R_{t+1}+\\gamma\\sum_{k=0}^\\infty\\gamma^kR_{t+k+2}\\mid S_t=s\\right]\\\\\n",
    "\\end{align}$$\n",
    "\n",
    "  * ta wartość oczekiwana zależy od __kolejnego stanu $s'$__, a więc od podjętej w stanie $s$ akcji $a$ i rewardów z tym związanych\n",
    "$$\\begin{align}\n",
    "v_\\pi(s)&=\\mathbb{E}_\\pi\\left[R_{t+1}+\\gamma\\sum_{k=0}^\\infty\\gamma^kR_{t+k+2}\\mid S_t=s\\right]\\\\\n",
    "&=\\sum_a\\pi(a\\mid s)\\sum_{s',r}p(s',r\\mid s,a)\\left[r+\\gamma\\left[\\sum_{k=0}^\\infty\\gamma^kR_{t+k+2}\\mid S_{t+1}=s'\\right]\\right]\\\\\n",
    "&=\\sum_a\\pi(a\\mid s)\\sum_{s',r}p(s',r\\mid s,a)\\left[r+\\gamma{}v_\\pi(s')\\right]\n",
    "\\end{align}$$\n",
    "  * to jest __równanie Bellmana__\n",
    "    * spojrzenie __wprzód__ z aktualnego stanu do __możliwych__ następników\n",
    "    * agent wybiera jedną z możliwych akcji $a$\n",
    "    * __środowisko__ odpowiada jednym z możliwych stanów $s'$ oraz rewardem $r$\n",
    "  * równanie Bellmana waży prawdopodobieństwa wystąpienia stanowiąc __kryterium optymalności__\n",
    "  * $v_\\pi$ jest jedynym mozliwym rozwiązaniem\n",
    "  * większość algorytmów RL będzie się starało rozwiązać równanie w jakiś sposób"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Kryterium optymalności Bellmana\n",
    "1. Równanie Bellmana stanowi __kryterium optymalności__ dla state-value i action-value funkcji\n",
    "  * state value\n",
    "  $$\\begin{align}\n",
    "v_\\ast(s)&=\\underset{a}{\\max}q_{\\pi_\\ast}(s, a)\\\\\n",
    "&=\\underset{a}{\\max}\\sum_{s',r}p(s',r\\mid s,a)\\left[r+\\gamma{}v_\\pi(s')\\right]\n",
    "\\end{align}$$\n",
    "\n",
    "  <img style=\"float: center;\" src=\"../mum_figures/bellman_backup.png\" width=\"75%\">\n",
    "\n",
    "  * action value\n",
    "  $$\\begin{align}\n",
    "q_\\ast(s,a)&=\\mathbb{E}\\left[R_{t+1}+\\gamma\\underset{a'}{\\max}q_\\ast(S_{t+1},a')\\mid S_t=s, A_t=a\\right]\\\\\n",
    "&=\\sum_{s',r}p(s',r\\mid s,a)\\left[r+\\gamma\\underset{a'}{\\max}q_\\ast(s',a')\\right]\n",
    "\\end{align}$$\n",
    "\n",
    "2. możliwa jest __ewaluacja policy__ przez iteracyjne dochodzenie do rozwiązania optymalnego\n",
    "  * action-value to __policy improvement__\n",
    "    * powolna operacja wymagająca w cyklach\n",
    "      * ewaluacji wszystkich akcji\n",
    "      * poprawy \n",
    "  * __value iteration__ jest prostsze\n",
    "  $$\\begin{align}\n",
    "v_{k+1}(s)&=\\underset{a}{\\max}\\mathbb{E}\\left[R_{t+1}+\\gamma{}v_k(S_{t+1})\\mid S_t=s, A_t=a\\right]\\\\\n",
    "&=\\underset{a}{\\max}\\sum_{s',r}p(s',r\\mid s,a)\\left[r+\\gamma{}v_k(s')\\right]\n",
    "\\end{align}$$\n",
    "    * odpowiada to przekształceniu równania Bellmana w regułę poprawy\n",
    "    \n",
    "  <img style=\"float: center;\" src=\"../mum_figures/bellman_backup_optimal.png\" width=\"75%\">\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
