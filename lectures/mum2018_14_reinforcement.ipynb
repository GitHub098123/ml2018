{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<big><big><big><big><big><big>Metody uczenia maszynowego</big></big></big></big></big></big>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<big><big><big><big><big>Reinforcement learning</big></big></big></big></big>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<id=tocheading><big><big><big><big>Spis tre≈õci</big></big></big></big>\n",
    "<div id=\"toc\"></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "\n",
    "from bokeh.io import gridplot, output_file, show\n",
    "from bokeh.plotting import figure, output_notebook\n",
    "from bkcharts import Scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"389b8ff2-bc7f-4af9-8034-9d39b4728e37\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      var el = document.getElementById(\"389b8ff2-bc7f-4af9-8034-9d39b4728e37\");\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"389b8ff2-bc7f-4af9-8034-9d39b4728e37\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '389b8ff2-bc7f-4af9-8034-9d39b4728e37' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.7.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.7.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.7.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.7.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "      document.getElementById(\"389b8ff2-bc7f-4af9-8034-9d39b4728e37\").textContent = \"BokehJS is loading...\";\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.7.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.7.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.7.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.7.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.7.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.7.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"389b8ff2-bc7f-4af9-8034-9d39b4728e37\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_notebook()\n",
    "sns.set(font_scale=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Image inclusion\n",
    "<img src=\"../mum_figures/\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Uczenie nadzorowane, nienadzorowane, ze wzmocnieniem\n",
    "1. __poznawcze__ pod _nadzorem_ nauczyciela podajƒÖcego prawid≈Çowe odpowiedzi (akcje do podjƒôcia)\n",
    "  * niepraktyczne w zadaniach interaktywnych\n",
    "2. __behawioralne__ przez interakcjƒô ze __≈õrodowiskiem__: uczenie __reinforcement__\n",
    "  * agent usi≈Çuje rozwiƒÖzaƒá zadanie\n",
    "  * w nieopisanym ≈õrodowisku agent musi uczyƒá siƒô z do≈õwiadczenia\n",
    "  * trudne (lub niemo≈ºliwe) zdobycie poprawnych odpowiedzi\n",
    "3. podej≈õcia do uczenia __ze wzmocnieniem__\n",
    "  * __klasyczne__ przez seriƒô nagr√≥d/kar dla osiƒÖgniƒôcia dobrego zachowania\n",
    "  * __wsp√≥≈Çczesne__ przez programowanie dynamiczne (__planowanie__)\n",
    "4. reinforcement jest r√≥≈ºne od nienadzorowanego\n",
    "  * RL stara siƒô maksymalizowaƒá __sygna≈Ç zwrotu__ (_ang_. __reward__)\n",
    "  * RL nie stara siƒô szukaƒá struktury problemu (chocia≈º mo≈ºe to byƒá przydatne)\n",
    "5. exploration - exploitation tradeoff\n",
    "  * __exploracja__ przeglƒÖdanie przestrzeni rozwiƒÖza≈Ñ w poszukiwaniu nowych\n",
    "  * __eksploatacja__ maksymalne wykorzystanie ju≈º znalezionych\n",
    "  * niemo≈ºliwe jest skupienie siƒô wy≈ÇƒÖczniena jednym "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elementy uczenia ze wzmocnieniem\n",
    "1. __polityka__ (_ang_. __policy__) definiuje spos√≥b zachowania agenta w danym momencie\n",
    "  * odwzorowanie ze stan√≥w do akcji do podjƒôcia w tym stanie\n",
    "  * tablica, albo proces wyszukiwania\n",
    "  * zwykle (czƒôsto) stochastyczne\n",
    "2. __sygna≈Ç wzmocnienia__ (_ang_. __reward__) okre≈õla cel\n",
    "  * w ka≈ºdym kroku ≈õrodowisko wysy≈Ça __pojedynczy__ sygna≈Ç wzmocnienia\n",
    "  * agent maksymalizuje __ca≈Çkowity__ reward\n",
    "  * na podstawie reward agnet moze zmieniƒá swojƒÖ politykƒô\n",
    "  * agent __nie ma__ wp≈Çywu na sam spos√≥b generowania sygna≈Çu \n",
    "  * reward to (stochastyczna) funkcja stanu i podjƒôtej akcji  \n",
    "3. funkcja __warto≈õci__ (_ang_. __value__) stanu\n",
    "  * okre≈õla co jest korzystne dla agenta __na d≈Çu≈ºszƒÖ__ metƒô\n",
    "  * warto≈õƒá stanu, to sumaryczne (warto≈õƒá oczekiwana) wzmocnienie (reward), kt√≥re agent mo≈ºe zdobyƒá w przysz≈Ço≈õci __poczƒÖwszy__ od tego stanu\n",
    "    * jesli baterie odkurzacza sƒÖ prawie na≈Çadowane, to warto≈õƒá tego stanu jest wy≈ºsza\n",
    "  * reward stanu mo≈ºe byƒá niskie (zmƒôczenie organizmu) ale warto≈õƒá stanu wysoka (ju≈º blisko szczytu)\n",
    "    * i odwrotnie\n",
    "  * warto≈õƒá przeliczana przez ca≈Çy okres dzia≈Çania agenta\n",
    "4. model ≈õrodowiska\n",
    "  * symuluje dzia≈Çanie rzeczywistego ≈õrodowiska"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przyk≈Çady (Sutton)\n",
    "1. gra w szachy i wyb√≥r ruchu \n",
    "  * zale≈ºny od sytuacji na planszy\n",
    "  * prawdopodobnych odpowiedzi na wiele ruch√≥w wprz√≥d\n",
    "2. kontroler maszyn w czasie rzeczywistym\n",
    "  * zale≈ºny od kosztu i zysk√≥w\n",
    "  * granicznych warunk√≥w\n",
    "3. m≈Çody gepard (gazela, robot, ...) wstajacy na nogi zaraz po urodzeniu\n",
    "4. samojezdny odkurzacz\n",
    "  * stan baterii\n",
    "  * stan ≈õrodowiska\n",
    "  * czƒô≈õciowa wiedza o ≈õrodowisku\n",
    "5. i wiele innych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gra w k√≥≈Çko i krzy≈ºyk\n",
    "1. minimax nie znajdzie optymalnej startegii\n",
    "  * wybiera akcjƒô, po kt√≥rej, przy __najlepszym__ ruchu przeciwnika, __nie__ przegra\n",
    "2. programowanie dynamiczne wymaga __pe≈Çnego__ opisu ≈õrodowiska (stan√≥w, akcji, prawdopodobie≈Ñstw)\n",
    "3. reinforcement z funkcjƒÖ warto≈õci stan√≥w\n",
    "  * wszystkie stany wygrane uzyskujƒÖ warto≈õƒá $1$\n",
    "  * przegrywajƒÖce $0$\n",
    "  * wszystkie inne $0.5$ \n",
    "    * brak innej wiedzy\n",
    "  * RL rozpoczyna grƒô z przeciwnikiem\n",
    "    * wiƒôkszo≈õƒá ruch√≥w __zach≈Çannych__: do stanu o __wy≈ºszej__ warto≈õci; czasem losowe\n",
    "      * eksploracja -- eksploatacja\n",
    "  * stany zmieniajƒÖ warto≈õƒá\n",
    "    $$V(s)=V(s)+\\alpha\\left[V(s')-V(s)\\right]$$      * to eksploatacja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem k-jednorƒôkich bandyt√≥w\n",
    "1. k-jednorƒôkich bandyt√≥w\n",
    "  * k maszyn\n",
    "  * ka≈ºda ma jakƒÖ≈õ __nieznanƒÖ__ stopƒô zwrotu\n",
    "  * akcja: wyb√≥r maszyny\n",
    "  * reward: ile maszyna zwr√≥ci po pociƒÖgniƒôciu wajchy\n",
    "  * algorytm $\\epsilon$-zach≈Çanny\n",
    "  $$A=\\begin{cases}\n",
    "  \\underset{a}{\\arg\\max}Q(a)&\\text{z prawdopodobie≈Ñstwem}\\hskip1em 1-\\epsilon\\\\\n",
    "  losowa&\\text{z prawdopodobie≈Ñstwem}\\hskip1em \\epsilon\n",
    "  \\end{cases}$$\n",
    "    * eksploracja - eksploatacja\n",
    "  * wyniki bƒôdƒÖ r√≥≈ºne w zale≈ºno≈õci od warto≈õci $\\epsilon$\n",
    "    * $\\epsilon=0$ - zach≈Çanny\n",
    "      * z poczƒÖtku ro≈õnie szybciej, by szybko wyhamowaƒá ju≈º bez zmian\n",
    "      * wybiera nieoptymalne akcje\n",
    "      * optymalne akcje tylko w u≈Çamku do≈õwiadcze≈Ñ\n",
    "    * $\\epsilon>0$\n",
    "      * tu algorytm wciƒÖ≈º eksploruje\n",
    "      * wiƒôksze $\\epsilon$ daje wiƒôkszƒÖ eksploracjƒô, jednak w ko≈Ñcu zachowuje siƒô gorzej\n",
    "      * mniejsze $\\epsilon$ uczy siƒô wolniej, ale daje w ko≈Ñcu lepsze wyniki\n",
    "        * mo≈ºna zmniejszaƒá $\\epsilon$\n",
    "    * zysk $\\epsilon$-zach≈Çannego zalezy bardzo od zadania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estymacja ≈õredniej stopy zwrotu\n",
    "2. estymacja ≈õredniej stopy zwrotu\n",
    "  $$\\begin{align}\n",
    "  Q_{n+1}&=\\frac{1}{n}\\sum_{t=1}^nR_t=\\frac{1}{n}\\left[R_n+\\sum_{t=1}^{n-1}R_t\\right]\\\\\n",
    "  &=\\frac{1}{n}\\left[R_n+(n-1)Q_n\\right]=\\frac{1}{n}\\left[R_n+nQ_n-Q_n\\right]\n",
    "  =Q_n+\\frac{1}{n}\\left[R_n-Q_n\\right]\n",
    "  \\end{align}$$\n",
    "  * $\\alpha=\\frac{1}{n}$ dla ≈õrodowiska __stacjonarnego__ gdy bandyci nie zmieniajƒÖ siƒô\n",
    "3. Estymacja dla $\\alpha$\n",
    "  $$\\begin{align}\n",
    "  Q_{n+1}&=Q_n+\\alpha\\left[R_n-Q_n\\right]=\\alpha{}R_n+(1-\\alpha)Q_n\\\\\n",
    "  &=\\alpha{}R_n+(1-\\alpha)\\left[\\alpha{}R_{n-1}+(1-\\alpha)Q_{n-1}\\right]\\\\\n",
    "  &=\\dots\\\\\n",
    "  &=\\alpha{}R_n+(1-\\alpha)\\alpha{}R_{n-1}+\\dots+(1-\\alpha)^{n-1}\\alpha{}R_{1}+(1-\\alpha)^nQ_1\\\\\n",
    "  &=(1-\\alpha)^nQ_1+\\sum_{t=1}^n\\alpha(1-\\alpha)^{n-t}R_t\n",
    "  \\end{align}$$\n",
    "  * suma wa≈ºona dla $\\alpha\\in(0,1]$\n",
    "  * waga $\\alpha(1-\\alpha)^{n-t}$ zale≈ºy od liczby obserwacji wstecz\n",
    "  * je≈õli $\\alpha=11$, to $1-\\alpha=0$ i wszystkie istotne wagi idƒÖ do ostatniej reward $R_n$\n",
    "    * konwencja: $0^0=1$\n",
    "  \n",
    "  * je≈õli __parametr kroku__ $\\alpha$ bƒôdzie __odpowiednio__ zmniejszany, to algorytm zbie≈ºny\n",
    "  $$\\sum_{k=1}^\\infty\\alpha_n(a)=\\infty\\hskip1em\\text{oraz}\\hskip1em\\sum_{k=1}^\\infty\\alpha_n^2(a)<\\infty$$\n",
    "    * niespe≈Çnienie drugiego warunku mo≈ºe byƒá przydatne w __niestacjonarnym__ (czy niestabilnym) ≈õrodowisku\n",
    "  * to metoda __temporal difference__\n",
    "4. optymistyczne warto≈õci poczƒÖtkowe\n",
    "  * standardowo $Q_1=0$\n",
    "  * optymistyczna warto≈õƒá wiƒôksza od rzeczywistej spowoduje wiƒôkszƒÖ eksploracjƒô\n",
    "  * praktyczne dla stacjonarnych, mo≈ºe zaburzyƒá dla niestacjonarnych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "### Upper confidence bound\n",
    "1. Wyb√≥r akcje wed≈Çug $$A_t=\\underset{a}{\\arg\\max}\\left[Q_t(a)+c\\sqrt{\\frac{\\log\\,t}{N_t(a)}}\\,\\right]$$\n",
    "gdzie dwa sk≈Çadniki \n",
    "  * $Q_t(a)$ to estymacja warto≈õci akcji $a$\n",
    "  * miara niepewno≈õci (wariancji) $\\sqrt{\\frac{\\log\\,t}{N_t(a)}}$\n",
    "2. UCB radzi sobie bardzo dobrze w zadaniach typu k-bandyt√≥w  \n",
    "  * mo≈ºe byƒá s≈Çabiej gdy zadania sƒÖ niestacjonarne\n",
    "  * czasem problem z radzeniem sobie z ca≈ÇƒÖ przestrzeniƒÖ rozwiƒÖza≈Ñ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upper Confidence bound applied to Trees UCT (albo MCTS - Monte Carlo Tree Search)\n",
    "  * drzewo gry\n",
    "  * przeciwnicy wybierajƒÖ ruch na zmianƒô\n",
    "  * w ka≈ºdym cyklu rozwijajƒÖc drzewo\n",
    "    1. gracz wybiera optymalny wierzcho≈Çek wyd≈Çug miary UCB\n",
    "    2. z wybranego wƒôz≈Ça (li≈õcia) wykonuje wszystkie mo≈ºliwe ruchy\n",
    "    3. dla ka≈ºdego z nich wykonuje __losowe symulacje__ gier zapamiƒôtujƒÖc wyniki\n",
    "    4. poprawia warto≈õci $Q_t(a)$\n",
    "      * dla li≈õcia u≈õrednia wyniki wszystkich gier i zapisuje z $N_t(a)=1$\n",
    "      * poprawia warto≈õci $Q_t(A_t),\\;t=t-1,\\dots,0$ po drodze a≈º do korzenia\n",
    "  * dobrze sprawdza siƒô dla gier\n",
    "  * przeszukuje w drzewie __tylko__ poddrzewa, kt√≥re dajƒÖ nadzieje\n",
    "  * jeden z pierwszych algorytm√≥w, kt√≥ry da≈Ç w miarƒô sensowne wyniki w Go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metody gradientowe (!?!?!?!?!?!!!!?)\n",
    "1. Podstawowe metody estymujƒÖ warto≈õci akcji i na tej podstawie wybierajƒÖ najlepszƒÖ\n",
    "2. mo≈ºna zdefiniowaƒá __preferencjƒô__ dla akcji $H_t(a)$\n",
    "  * preferencja __nie ma__ interpretacji w sensie nagrody (reward)\n",
    "  * im wy≈ºsza preferencja __wzglƒôdem innych__ tym czƒô≈õciej bƒôdzie wybrana\n",
    "  \n",
    "  $$\\pi_t(a)=p(A_t=a)=\\frac{\\exp(H_t(a))}{\\sum_b\\exp(H_t(b))}$$\n",
    "3. Algorytm __stochastycznego wzrostu gradientu__\n",
    "\n",
    "  $$\\begin{align}\n",
    "  H_{t+1}&=H_{t}+\\alpha\\frac{\\partial\\mathbb{E}[R_t]}{\\partial{}H_t(a)}\\\\\n",
    "  \\mathbb{E}[R_t]&=\\sum_b\\pi_t(b)q_\\ast(b)\n",
    "  \\end{align}$$\n",
    "  gdzie $\\mathbb{E}[R+t]$ jest miarƒÖ jako≈õci, a $q_\\ast(b)$ jest (nieznanƒÖ) __optymalnƒÖ__ funkcjƒÖ state-value\n",
    "  * pochodna jako≈õci dzia≈Çania wzglƒôdem __preferencji__\n",
    "4. jakƒÖ postaƒá ma gradient? jak policzyƒá pochodnƒÖ oczekiwanej efektywno≈õci po preferencji $H_t(a)\\dots$?\n",
    "\n",
    "  $$\\begin{align}\n",
    "\\frac{\\partial\\mathbb{E}[R_t]}{\\partial{}H_t(a)}&=\\frac{\\partial}{\\partial{}H_t(a)}\\sum_bq_\\ast\\pi_t(b)=\\sum_bq_\\ast\\frac{\\partial\\pi_t(b)}{\\partial{}H_t(a)}\\\\\n",
    "&=\\sum_bq_\\ast\\frac{\\partial}{\\partial{}H_t(a)}\\frac{\\exp(H_t()b)}{\\sum_c\\exp(H_t(c))}\\\\\n",
    "&\\\\\n",
    "&\\hskip3em\\text{mo≈ºna odjƒÖƒá warto≈õƒá skalarnƒÖ $X_t$ niezale≈ºnƒÖ od $b$}\\\\\n",
    "&\\hskip3em\\text{bo $\\pi_t()$ sumujƒÖ siƒô do $1$, a gradienty do $0$}\\\\\n",
    "&\\\\\n",
    "&=\\sum_b(q_\\ast-X_t)\\frac{\\partial}{\\partial{}H_t(a)}\\frac{\\exp(H_t(b))}{\\sum_c\\exp(H_t(c))}\\\\\n",
    "&\\\\\n",
    "&\\hskip3em\\text{gradient $\\pi_t(b)$, jako funkcji softmax, ma postaƒá}\\\\\n",
    "&\\hskip5em\\frac{\\partial\\pi_t(b)}{\\partial{}H_t(a)}=\\pi_t(b)[1_{a=b}-\\pi_t(a)]\\\\\n",
    "&\\\\\n",
    "&=\\sum_b(q_\\ast-X_t)\\,\\pi_t(b)\\,[1_{a=b}-\\pi_t(a)]\\\\\n",
    "&=\\mathbb{E}_\\pi\\left[(q_\\ast-X_t)[1_{a=b}-\\pi_t(a)]\\right]\\\\\n",
    "&\\\\\n",
    "&\\hskip3em\\text{poniewa≈º optymalna action-value $\\mathbb{E}[R_t\\mid A_t]=q_\\ast(A_t)$}\\\\\n",
    "&\\hskip3em\\text{niech $q_\\ast(A_t)=R_t$ ($R_t$ nieskorelowane)}\\\\\n",
    "&\\hskip3em\\text{oraz przyjƒÖƒá baseline $X_t=Q_{t+1}$ - ≈õredniej warto≈õci zwrotu}\\\\\n",
    "&\\\\\n",
    "&=\\mathbb{E}_\\pi\\left[(R_t-Q_{t+1})[1_{a=b}-\\pi_t(a)]\\right]\n",
    "\\end{align}$$\n",
    "\n",
    "  gdzie $Q_{t+1}=1/t\\sum_{k=1}^tR_k$\n",
    "5. W ten spos√≥b mamy __wzory na poprawƒô preferencji__\n",
    "  * dla wybranej akcji $A_t$ $$H_{t+1}(A_t)=H(A_t)+\\alpha(R_t-Q_{t+1})(1-\\pi_t(A_t))$$\n",
    "  * dla akcji niewybranych $a\\neq A_t$ $$H_{t+1}(a)=H(A_t)-\\alpha(R_t-Q_{t+1})\\pi_t(a)$$\n",
    "  * sk≈Çadnik $Q_{t+1}$ s≈Çu≈ºy jako __odniesienie__ z kt√≥rym por√≥wnywana jest warto≈õƒá $R_t$ uzyskana\n",
    "    * je≈õli $R_t$ jest __wiƒôksza__, to preferencja bƒôdzie __zwiƒôkszana__\n",
    "    * preferencje __nie wybranych__ akcji bƒôdƒÖ zmniejszane __proporcjonalnie__ do szans ich wybrania $\\pi_t(a)$\n",
    "  * ominiƒôcie sk≈Çadnika odniesienia wp≈Çywa os≈ÇabiajƒÖco na algorytm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meƒô√®√©√™√´ƒóƒìto√≥√¥√∂√≤√µ≈ì√∏≈çdy (niekt√≥re)\n",
    "(bez okre≈õlonego porzƒÖdku)\n",
    "1. metody ewolucyjne\n",
    "  * symulujƒÖ zachowanie agent√≥w w czasie ca≈Çego ich ≈ºycia\n",
    "  * nie okre≈õlajƒÖ warto≈õci stan√≥w\n",
    "  * nie kontaktujƒÖ siƒô ze ≈õrodowiskiem\n",
    "  * abstrahujƒÖ od definicji policy, kt√≥rej szukajƒÖ, jako funkcji stan√≥w w akcje\n",
    "2. przeszukiwanie przestrzeni rozwiƒÖza≈Ñ\n",
    "  * optymalizacja policy jako zbioru numerycznych parametr√≥w i poszukiwanie kierunku poprawy\n",
    "  * __policy gradient__\n",
    "3. przeszukiwanie metodami Monte-Carlo\n",
    "  * okre≈õlenie lepszych/gorszych obszar√≥w\n",
    "  * optymalizacja dylematu exploration-exploitation\n",
    "  * __upper confidence bound__\n",
    "  * tak≈ºe proste metody losowe i optymalizacji __hill climbing__\n",
    "4. __programowanie dynamiczne__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Law of Effect\n",
    "_Of several responses made to the same situation, those which are accompanied or closely followed by satisfaction to the animal will, other things being equal, be more firmly connected with the situation, so that, when it recurs, they will be more likely to recur; those which are accompanied or closely followed by discomfort to the animal will, other things being equal, have their connections with this situation weakened, so that, when it recurs, they will be less likely to occur. The greater the satisfaction or discomfort, the greater the strengthening or weakening of the bond_ (Edward Thorndike, 1911)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit assignment problem\n",
    "1. __programowanie dynamiczne__\n",
    "  * decyzje podejmowane w krokach\n",
    "  * ≈ºƒÖdanie niskiego __aktualnego__ kosztu musi byƒá zr√≥wnowa≈ºone z __wysokim__ kosztem w przysz≈Ço≈õci\n",
    "  * kredyt musi byƒá zr√≥wnowa≈ºony przez wszystkie podejmowane decyzje\n",
    "  \n",
    "<big>___W jaki spos√≥b agent mo≈ºe podnie≈õƒá swoje d≈Çugo-terminowe wyniki w stochastycznym ≈õrodowisku gdy osiƒÖgniƒôcie zysk√≥w mo≈ºe siƒô wiƒÖzaƒá kosztem wynik√≥w kr√≥tko-terminowych?___</big>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesy decyzyjne Markowa\n",
    "1. agent dzia≈Ça zgodnie z za≈Ço≈ºeniami __sko≈Ñczenie stanowego procesu Markowa__\n",
    "  <img style=\"float: right;\" src=\"../mum_figures/markov_process.png\" width=\"45%\">\n",
    "  * proces rozwija siƒô __probabilistycznie__\n",
    "  * ma __sko≈ÑczonƒÖ__ liczbƒô stan√≥w\n",
    "  * __nie zawiera__ statystyk poprzedniego zachowania\n",
    "  * dla ka≈ºdego stanu istnieje __sko≈Ñczona__ liczba akcji mo≈ºliwych do podjƒôcia\n",
    "  * ka≈ºda akcja jest zwiƒÖzana z pewnym __kosztem__\n",
    "  * wszystko dzieje siƒô w czasie __dyskretnym__\n",
    "  \n",
    "    __Stan ≈õrodowiska jest pewnym _podsumowaniem_ ca≈Çego dotychczasowego do≈õwiadczenia agenta z tym ≈õrodowiskiem. Informacja konieczna dla agenta dla przewidywania przysz≈Ço≈õci jest zawarta w tym podsumowaniu__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Akcje\n",
    "2. dla ka≈ºdego stanu $s$ istnieje __sko≈Ñczony__ zbi√≥r akcji $a_{s,s'}$\n",
    "  <img style=\"float: right;\" src=\"../mum_figures/markov_process.png\" width=\"45%\">\n",
    "  * akcja przeprowadza __probabilistycznie__ stan $S_t=s$ w stan $S_{n+1}=s'$  \n",
    "    1. $p_{s,s'}(a)\\ge0$\n",
    "    2. $\\sum_{s'}p_{s,s'}(a)=1$\n",
    "  ---\n",
    "<big>__W≈Çasno≈õƒá Markowa: prawdopodobie≈Ñstwo przej≈õcia ze stanu $s$ w stan $s'$ zale≈ºy _wy≈ÇƒÖcznie_ od aktualnego stanu $s$ oraz akcji $a_{s,s'}$__</big>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward\n",
    "3. ka≈ºde przej≈õcie jest zwiƒÖzane z reward (kosztem) $r(s, a, s')$ dla agenta\n",
    "<img style=\"float: right;\" src=\"../mum_figures/markov_process.png\" width=\"45%\">  \n",
    "  * $r(s, a_{s,s'}, s')$ jest ustalonƒÖ funkcjƒÖ\n",
    "  * szachy: +1 za wygranƒÖ, -1 za przegranƒÖ, 0 w innym przypadku\n",
    "  * odkurzacz: \n",
    "    * dodatnie warto≈õci za odkurzanie dawno nie odwiedzanego terenu\n",
    "    * dodatnie ale mniejsze za odkurzanie terenu widzanego niedawno\n",
    "    * zerowa za utrzymywanie baterii w stanie na≈Çadowanym\n",
    "    * wysoka ujemna za roz≈Çadowanie baterii\n",
    "  * reward jest __deklaracjƒÖ celu__\n",
    "    * __nie jest__ deklaracjƒÖ sposobu jego osiƒÖgniƒôcia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zwrot (return)\n",
    "  <big>__Celem i zadaniem procesu jest maksymalizacja _oczekiwanej warto≈õci skumulowanej sumy_ uzyskanych warto≈õci skalarnych (reward)__</big>\n",
    "1. Celem jest maksymalizacja oczekiwanej warto≈õci skumulowanych reward\n",
    "  * najpro≈õciej $$G_t=R_{t+1}+R_{t+2}+\\dots+R_{T}$$\n",
    "  gdzie $T$ jest numerem ostatniego kroku\n",
    "2. __epizod__ oznacza naturalnƒÖ sko≈ÑczonƒÖ sekwencjƒô zdarze≈Ñ agent-≈õrodowisko\n",
    "  * pojedyncze gry, codzienne odkurzanie, etc.\n",
    "  * epizod ma stan ko≈Ñcowy (terminalny)\n",
    "    * po ko≈Ñcu nastƒôpuje __powr√≥t__ do ustalonego (lub okre≈õlanego inaczej, np. losowanego) stanu poczƒÖtkowego\n",
    "  * zadania w epizodach to, nomen omen, __zdania epizodyczne__\n",
    "3. zadania __ciƒÖg≈Çe__\n",
    "  * nie dzielƒÖ siƒô na epizody\n",
    "  * wtedy $$G_t=R_{t+1}+\\gamma{}R_{t+2}+\\gamma{}R_{t+3}\\dots=\\sum_{k=0}^\\infty\\gamma^kR_{t+k+1}$$\n",
    "  * $0\\ge\\gamma\\ge1$ jest czynnikiem __discount__\n",
    "  * regulacja $\\gamma$ pozwala sterowaƒá na ile agent interesuje siƒô przysz≈Ço≈õciƒÖ\n",
    "    * ma≈Çe gamma to zainteresowanie celami bliskimi\n",
    "    * $\\gamma=0$ daje agenta __kr√≥tkowzrocznego__ (ang. myopic) zainteresowanego __tylko__ chwilƒÖ obecnƒÖ\n",
    "    * dla $\\gamma$ bli≈ºszego $1$ wa≈ºniejsza staje siƒô przysz≈Ço≈õƒá (ca≈Ça historia)\n",
    "4. Zunifikowana notacja to $$G_t=\\sum_{k=0}^{T-t-1}\\gamma^kR_{t+k+1}$$ \n",
    "  * w tym $T=\\infty$\n",
    "  * a tak≈ºe $\\gamma=1$ (ale __nie jednocze≈õnie z $T=\\infty$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy (strategia, polityka)\n",
    "4. __policy__ $\\pi$ to __odwzorowanie stan√≥w w akcje__\n",
    "  * deterministyczne $\\pi(s)$ opisuje akcjƒô podejmowanƒÖ w stanie s\n",
    "  * stochastyczne $\\pi_t(a\\mid s)=P(A_t=a\\mid S_t=s)$\n",
    "  * policy jest __dopuszczalna__ (ang. admissible) je≈õli $\\pi(s)\\in\\mathcal{A}_s$ gdzie $\\mathcal{A}_s$ jest zbiorem wszystkich mo≈ºliwych akcji w stanie $s$\n",
    "  * policy mo≈ºe byƒá\n",
    "    * __niestacjonarna__ je≈õli zmienia siƒô w czasie\n",
    "    * __stacjonarna__ je≈õli jest niezale≈ºna od kroku\n",
    "5. Proces Markowa mo≈ºe byƒá\n",
    "  * __stacjonarny__, gdy $p_{s,s'}$ nie zmieniajƒÖ siƒô\n",
    "  * __niestacjonarny__ w przeciwnym wypadku"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funkcje warto≈õci (value)\n",
    "1. funkcja __warto≈õci__ (_ang_. __value__, __state-value__) $v_\\pi(s)$ okre≈õla jako≈õƒá stanu $s$ przy polityce (policy) $\\pi$\n",
    "  * czyli oczekiwana warto≈õƒá zwrotu w stanie $s$\n",
    "\n",
    "  $$\\begin{align}\n",
    "v_\\pi(s)&=\\mathbb{E}_\\pi\\left[G_t\\mid S_t=s\\right]\\\\\n",
    "&=\\mathbb{E}_\\pi\\left[\\sum_{k=0}^\\infty\\gamma^kR_{t+k+1}\\mid S_t=s\\right]\n",
    "\\end{align}$$\n",
    "2. podobnie warto≈õƒá podjƒôcia akcji (_ang_. __action-value__) $a$ w stanie $s$ w polityce $\\pi$ \n",
    "\n",
    "  $$\\begin{align}\n",
    "q_\\pi(s, a)&=\\mathbb{E}_\\pi\\left[G_t\\mid S_t=s, A_t=a\\right]\\\\\n",
    "&=\\mathbb{E}_\\pi\\left[\\sum_{k=0}^\\infty\\gamma^kR_{t+k+1}\\mid S_t=s, A_t=a\\right]\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R√≥wnanie Bellmana\n",
    "3. warto≈õci stanu u≈ºyte w trakcie uczenia spe≈ÇniajƒÖ zale≈ºno≈õci rekurencyjne\n",
    "\n",
    "  $$\\begin{align}\n",
    "v_\\pi(s)&=\\mathbb{E}_\\pi\\left[G_t\\mid S_t=s\\right]\\\\\n",
    "&=\\mathbb{E}_\\pi\\left[\\sum_{k=0}^\\infty\\gamma^kR_{t+k+1}\\mid S_t=s\\right]\\\\\n",
    "&=\\mathbb{E}_\\pi\\left[R_{t+1}+\\gamma\\sum_{k=0}^\\infty\\gamma^kR_{t+k+2}\\mid S_t=s\\right]\\\\\n",
    "\\end{align}$$\n",
    "\n",
    "  * ta warto≈õƒá oczekiwana zale≈ºy od __kolejnego stanu $s'$__, a wiƒôc od podjƒôtej w stanie $s$ akcji $a$ i reward√≥w z tym zwiƒÖzanych\n",
    "$$\\begin{align}\n",
    "v_\\pi(s)&=\\mathbb{E}_\\pi\\left[R_{t+1}+\\gamma\\sum_{k=0}^\\infty\\gamma^kR_{t+k+2}\\mid S_t=s\\right]\\\\\n",
    "&=\\sum_a\\pi(a\\mid s)\\sum_{s',r}p(s',r\\mid s,a)\\left[r+\\gamma\\left[\\sum_{k=0}^\\infty\\gamma^kR_{t+k+2}\\mid S_{t+1}=s'\\right]\\right]\\\\\n",
    "&=\\sum_a\\pi(a\\mid s)\\sum_{s',r}p(s',r\\mid s,a)\\left[r+\\gamma{}v_\\pi(s')\\right]\n",
    "\\end{align}$$\n",
    "  * to jest __r√≥wnanie Bellmana__\n",
    "    * spojrzenie __wprz√≥d__ z aktualnego stanu do __mo≈ºliwych__ nastƒôpnik√≥w\n",
    "    * agent wybiera jednƒÖ z mo≈ºliwych akcji $a$\n",
    "    * __≈õrodowisko__ odpowiada jednym z mo≈ºliwych stan√≥w $s'$ oraz rewardem $r$\n",
    "  * r√≥wnanie Bellmana wa≈ºy prawdopodobie≈Ñstwa wystƒÖpienia stanowiƒÖc __kryterium optymalno≈õci__\n",
    "  * $v_\\pi$ jest jedynym mozliwym rozwiƒÖzaniem\n",
    "  * wiƒôkszo≈õƒá algorytm√≥w RL bƒôdzie siƒô stara≈Ço rozwiƒÖzaƒá r√≥wnanie w jaki≈õ spos√≥b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Kryterium optymalno≈õci Bellmana\n",
    "1. R√≥wnanie Bellmana stanowi __kryterium optymalno≈õci__ dla state-value i action-value funkcji\n",
    "  * state value\n",
    "  $$\\begin{align}\n",
    "v_\\ast(s)&=\\underset{a}{\\max}q_{\\pi_\\ast}(s, a)\\\\\n",
    "&=\\underset{a}{\\max}\\sum_{s',r}p(s',r\\mid s,a)\\left[r+\\gamma{}v_\\pi(s')\\right]\n",
    "\\end{align}$$\n",
    "\n",
    "  <img style=\"float: center;\" src=\"../mum_figures/bellman_backup.png\" width=\"75%\">\n",
    "\n",
    "  * action value\n",
    "  $$\\begin{align}\n",
    "q_\\ast(s,a)&=\\mathbb{E}\\left[R_{t+1}+\\gamma\\underset{a'}{\\max}q_\\ast(S_{t+1},a')\\mid S_t=s, A_t=a\\right]\\\\\n",
    "&=\\sum_{s',r}p(s',r\\mid s,a)\\left[r+\\gamma\\underset{a'}{\\max}q_\\ast(s',a')\\right]\n",
    "\\end{align}$$\n",
    "\n",
    "2. mo≈ºliwa jest __ewaluacja policy__ przez iteracyjne dochodzenie do rozwiƒÖzania optymalnego\n",
    "  * action-value to __policy improvement__\n",
    "    * powolna operacja wymagajƒÖca w cyklach\n",
    "      * ewaluacji wszystkich akcji\n",
    "      * poprawy \n",
    "  * __value iteration__ jest prostsze\n",
    "  $$\\begin{align}\n",
    "v_{k+1}(s)&=\\underset{a}{\\max}\\mathbb{E}\\left[R_{t+1}+\\gamma{}v_k(S_{t+1})\\mid S_t=s, A_t=a\\right]\\\\\n",
    "&=\\underset{a}{\\max}\\sum_{s',r}p(s',r\\mid s,a)\\left[r+\\gamma{}v_k(s')\\right]\n",
    "\\end{align}$$\n",
    "    * odpowiada to przekszta≈Çceniu r√≥wnania Bellmana w regu≈Çƒô poprawy\n",
    "    \n",
    "  <img style=\"float: center;\" src=\"../mum_figures/bellman_backup_optimal.png\" width=\"75%\">\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
