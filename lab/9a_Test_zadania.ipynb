{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.metrics import accuracy_score as acc\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "from datasets import get_dataset, list_datasets, train_test_split\n",
    "from zadania import DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balance-scale {'X': 'categorical (ordered)', 'y': 'categorical (ordered)'}\n",
      "banknote {'X': 'continuous', 'y': 'categorical'}\n",
      "car {'X': 'categorical (ordered)', 'y': 'categorical (ordered)'}\n",
      "iris {'X': 'continuous', 'y': 'categorical'}\n",
      "wine {'X': 'mixed', 'y': 'continuous'}\n"
     ]
    }
   ],
   "source": [
    "balance = get_dataset(\"balance-scale\")\n",
    "banknote = get_dataset(\"banknote\")\n",
    "iris = get_dataset(\"iris\")\n",
    "wine = get_dataset(\"wine\")\n",
    "car = get_dataset(\"car\")\n",
    "\n",
    "for name in list_datasets():\n",
    "    print(name, get_dataset(name).dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Domyślne parametry sklearn'a:\n",
    "* `DecisionTreeClassifier(criterion=’gini’, splitter=’best’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort=False)`\n",
    "* `DecisionTreeRegressor(criterion=’mse’, splitter=’best’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, presort=False)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class DTR:\n",
    "    def __init__(self, X, y):\n",
    "        self.m = DecisionTreeRegressor()\n",
    "        self.m.fit(X, y)\n",
    "    def predict(self, X):\n",
    "        return self.m.predict(X)\n",
    "\n",
    "class DTC:\n",
    "    def __init__(self, X, y, criterion, max_depth=None, min_samples_split=2):\n",
    "        self.m = DecisionTreeClassifier(criterion=criterion, max_depth=max_depth, min_samples_split=min_samples_split)\n",
    "        self.m.fit(X, y)\n",
    "    def predict(self, X):\n",
    "        return self.m.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co należy przede wszystkim sprawdzić\n",
    "\n",
    "#### Wynik na zbiorze treningowym\n",
    "\n",
    "Jeśli podczas uczenia nie stosujemy pruningu i drzewo budowane jest do samego końca, to wynik na zbiorze treningowym musi wynosić 100% dla accuracy i 0. dla MSE, jeśli tylko w zbiorze treningowym nie ma dwóch identycznych x'ów o różnych y'ach (dlaczego?).\n",
    "\n",
    "#### Liczba liści, rozmiar liści, głębokość drzewa\n",
    "\n",
    "Dobrze jest dodać w `__init__` asserty, które po nauczeniu sprawdzają, czy:\n",
    "* głębokość drzewa nie przekracza maksymalnej dozwolonej,\n",
    "* suma liczb elementów we wszystkich liściach jest równa rozmiarowi zbioru treningowego,\n",
    "* nie ma pustych liści,\n",
    "* drzewo jest binarne (n_węzłów = n_liści - 1), chyba że robimy niebinarne splity, czego robić nie polecam,\n",
    "* rozmiar żadnego liścia nie przekracza maksymalnego rozmiaru.\n",
    "\n",
    "#### Kryterium splitu binarnego\n",
    "\n",
    "Jeśli w węźle jest $n$ danych, to należy sprawdzić $n-1$ (nie $n$) splitów. Nie może się okazać, że najlepszy split jest gorszy od braku splitu (uważać na wzory z wariancją!). Nie można podzielić node'a o rozmiarze $n$ na dwa node'y o rozmiarach odpowiednio $n$ oraz $0$.\n",
    "\n",
    "Jeśli w węźle są dwa identyczne wiersze feature'ów $x_1 = x_2$, to nie ma jak wykonać na nich splitu, nawet jeśli $y_1 \\neq y_2$ i ten fakt trzeba uwzględnić przy sprawdzaniu warunków utworzenia liścia!\n",
    "\n",
    "Jeśli w węźle wszystkie $y$ są identyczne, to teoretycznie można dalej go splitować, ale otrzymane w ten sposób drzewo będzie __równoważne__ (dlaczego?) drzewu, w którym przerywamy splitowanie i tworzymy liść.\n",
    "\n",
    "#### Wartości domyślne cech w drzewach uczonych na Categorical Dataset\n",
    "\n",
    "Jeśli features w tablicy `X` są typu categorical/discrete, a nie continuous, to w zbiorze testowym mogą pojawić się __wartości__ pewnej cechy - niech dla ustalenia uwagi będzie to pierwsza kolumna `X` - które nie wystąpiły ani razu w zbiorze treningowym. W takim wypadku jeśli podczas predykcji dotrzemy do node'a, który wykonuje split po pierwszej kolumnie, to drzewo nie będzie wiedziało, jak zaklasyfikować dany przykład.\n",
    "\n",
    "Z tego powodu drzewa uczone na danych categorical powinny mieć w każdym węźle zdefiniowaną ścieżkę domyślną - najlepiej (chyba) wybrać tę ścieżkę, która miała najwięcej przykładów w zbiorze treningowym, czyli w pewnym sensie jest najbardziej prawdopodobna.\n",
    "\n",
    "Rozwiązanie z domyślną ścieżką jest o tyle dobre, że być może w dalszej części drzewa zostaną wzięte pod uwagę pozostałe cechy danego przykładu.\n",
    "\n",
    "Dlaczego ten problem nie zachodzi dla danych `X` typu continuous?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def eval(model_cls, model_kwargs, dataset, fn_score):\n",
    "    split = train_test_split(dataset)\n",
    "    model = model_cls(split.train.X, split.train.y, **model_kwargs)\n",
    "    score = {\n",
    "        \"train\": fn_score(split.train.y, model.predict(split.train.X)),\n",
    "        \"test\": fn_score(split.test.y, model.predict(split.test.X)),\n",
    "    }\n",
    "    return {\n",
    "        \"split\": split,\n",
    "        \"model\": model,\n",
    "        \"score\": score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn DTC with 'gini': {'train': 1.0, 'test': 0.98783454987834551}\n",
      "sklearn DTC with 'entropy': {'train': 1.0, 'test': 0.99026763990267641}\n",
      "DecisionTree: {'train': 1.0, 'test': 0.97810218978102192}\n",
      "Tree structure:\n",
      " f: 0 thr: 0.30081 pred: 0\n",
      "- f: 1 thr: 5.8333 pred: 1\n",
      "-- f: 2 thr: 2.9986 pred: 1\n",
      "--- pred: 1\n",
      "--- f: 1 thr: -1.8624 pred: 1\n",
      "---- f: 0 thr: -0.69879 pred: 1\n",
      "----- pred: 1\n",
      "----- f: 0 thr: -0.64472 pred: 1\n",
      "------ pred: 0\n",
      "------ pred: 1\n",
      "---- f: 0 thr: -2.5919 pred: 0\n",
      "----- pred: 1\n",
      "----- pred: 0\n",
      "-- f: 0 thr: -4.2249 pred: 0\n",
      "--- pred: 1\n",
      "--- pred: 0\n",
      "- f: 0 thr: 2.3917 pred: 0\n",
      "-- f: 2 thr: -1.7859 pred: 0\n",
      "--- f: 1 thr: 4.8731 pred: 1\n",
      "---- pred: 1\n",
      "---- pred: 0\n",
      "--- f: 0 thr: 0.75896 pred: 0\n",
      "---- f: 3 thr: -0.068117 pred: 0\n",
      "----- f: 2 thr: -1.4501 pred: 0\n",
      "------ f: 0 thr: 0.40614 pred: 0\n",
      "------- pred: 1\n",
      "------- pred: 0\n",
      "------ pred: 0\n",
      "----- f: 2 thr: 1.7048 pred: 1\n",
      "------ pred: 1\n",
      "------ pred: 0\n",
      "---- pred: 0\n",
      "-- pred: 0\n"
     ]
    }
   ],
   "source": [
    "# GINI VS ENTROPY ON CONTINUOUS FEATURES\n",
    "\n",
    "#dataset = balance\n",
    "dataset = banknote\n",
    "#dataset = car\n",
    "#dataset = iris\n",
    "\n",
    "print(\"sklearn DTC with 'gini':\", eval(DTC, {\"max_depth\": None, \"criterion\": \"gini\"}, dataset, acc)[\"score\"])\n",
    "print(\"sklearn DTC with 'entropy':\", eval(DTC, {\"max_depth\": None, \"min_samples_split\": 2, \"criterion\": \"entropy\"}, dataset, acc)[\"score\"])\n",
    "evaluated = eval(DecisionTree, {\"max_depth\": None, \"min_size\": 2}, dataset, acc)\n",
    "print(\"DecisionTree:\", evaluated[\"score\"])\n",
    "print(\"Tree structure:\")\n",
    "print(evaluated[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn DTR: {'train': 0.0, 'test': 50777.981132075474}\n",
      "DecisionTree: {'train': 0.0, 'test': 56809.037735849059}\n",
      "Tree structure:\n",
      " f: 0 thr: 1.0 pred: 735.576\n",
      "- f: 10 thr: 6.2 pred: 1119.41025641\n",
      "-- f: 1 thr: 13.28 pred: 1025.0\n",
      "--- f: 2 thr: 2.05 pred: 827.857142857\n",
      "---- f: 1 thr: 12.85 pred: 902.5\n",
      "----- pred: 1015.0\n",
      "----- f: 2 thr: 1.77 pred: 865.0\n",
      "------ f: 1 thr: 13.05 pred: 882.5\n",
      "------- pred: 885.0\n",
      "------- pred: 880.0\n",
      "------ pred: 830.0\n",
      "---- f: 2 thr: 3.8 pred: 728.333333333\n",
      "----- f: 1 thr: 12.93 pred: 752.5\n",
      "------ pred: 770.0\n",
      "------ pred: 735.0\n",
      "----- pred: 680.0\n",
      "--- f: 12 thr: 3.58 pred: 1087.72727273\n",
      "---- f: 3 thr: 2.41 pred: 1115.25\n",
      "----- f: 5 thr: 94.0 pred: 1035.5\n",
      "------ f: 1 thr: 13.3 pred: 1217.5\n",
      "------- pred: 1285.0\n",
      "------- pred: 1150.0\n",
      "------ f: 1 thr: 13.56 pred: 990.0\n",
      "------- f: 1 thr: 13.48 pred: 857.5\n",
      "-------- pred: 920.0\n",
      "-------- pred: 795.0\n",
      "------- f: 1 thr: 13.9 pred: 1034.16666667\n",
      "-------- f: 2 thr: 1.67 pred: 1011.66666667\n",
      "--------- pred: 1060.0\n",
      "--------- f: 1 thr: 13.68 pred: 987.5\n",
      "---------- pred: 990.0\n",
      "---------- pred: 985.0\n",
      "-------- f: 1 thr: 14.38 pred: 1056.66666667\n",
      "--------- f: 1 thr: 14.1 pred: 1062.5\n",
      "---------- pred: 1060.0\n",
      "---------- pred: 1065.0\n",
      "--------- pred: 1045.0\n",
      "----- f: 10 thr: 4.5 pred: 1195.0\n",
      "------ f: 1 thr: 13.51 pred: 1005.0\n",
      "------- pred: 1095.0\n",
      "------- pred: 915.0\n",
      "------ f: 11 thr: 0.92 pred: 1242.5\n",
      "------- f: 1 thr: 13.39 pred: 1145.0\n",
      "-------- pred: 1195.0\n",
      "-------- pred: 1095.0\n",
      "------- f: 5 thr: 121.0 pred: 1275.0\n",
      "-------- f: 5 thr: 94.0 pred: 1283.0\n",
      "--------- pred: 1265.0\n",
      "--------- f: 3 thr: 2.61 pred: 1287.5\n",
      "---------- f: 1 thr: 14.06 pred: 1292.5\n",
      "----------- pred: 1295.0\n",
      "----------- pred: 1290.0\n",
      "---------- f: 1 thr: 13.73 pred: 1282.5\n",
      "----------- pred: 1285.0\n",
      "----------- pred: 1280.0\n",
      "-------- pred: 1235.0\n",
      "---- f: 1 thr: 13.5 pred: 812.5\n",
      "----- pred: 845.0\n",
      "----- pred: 780.0\n",
      "-- f: 1 thr: 13.94 pred: 1393.2\n",
      "--- f: 7 thr: 3.19 pred: 1295.83333333\n",
      "---- f: 1 thr: 13.58 pred: 1445.0\n",
      "----- pred: 1515.0\n",
      "----- pred: 1375.0\n",
      "---- f: 4 thr: 14.0 pred: 1221.25\n",
      "----- f: 1 thr: 13.05 pred: 1170.0\n",
      "------ pred: 1150.0\n",
      "------ pred: 1190.0\n",
      "----- f: 1 thr: 13.72 pred: 1272.5\n",
      "------ pred: 1285.0\n",
      "------ pred: 1260.0\n",
      "--- f: 1 thr: 14.19 pred: 1539.25\n",
      "---- pred: 1680.0\n",
      "---- f: 1 thr: 14.37 pred: 1492.33333333\n",
      "----- f: 1 thr: 14.2 pred: 1465.0\n",
      "------ pred: 1450.0\n",
      "------ pred: 1480.0\n",
      "----- pred: 1547.0\n",
      "- f: 5 thr: 100.0 pred: 561.511627907\n",
      "-- f: 6 thr: 2.3 pred: 514.533333333\n",
      "--- f: 8 thr: 0.61 pred: 555.3\n",
      "---- f: 4 thr: 16.8 pred: 543.078947368\n",
      "----- f: 2 thr: 0.94 pred: 442.5\n",
      "------ pred: 520.0\n",
      "------ f: 1 thr: 12.64 pred: 416.666666667\n",
      "------- f: 1 thr: 11.66 pred: 439.0\n",
      "-------- pred: 428.0\n",
      "-------- pred: 450.0\n",
      "------- pred: 372.0\n",
      "----- f: 9 thr: 1.55 pred: 554.911764706\n",
      "------ f: 9 thr: 0.68 pred: 579.260869565\n",
      "------- f: 1 thr: 13.52 pred: 468.333333333\n",
      "-------- f: 1 thr: 12.77 pred: 495.0\n",
      "--------- pred: 470.0\n",
      "--------- pred: 520.0\n",
      "-------- pred: 415.0\n",
      "------- f: 2 thr: 1.67 pred: 595.9\n",
      "-------- f: 3 thr: 2.24 pred: 542.285714286\n",
      "--------- f: 3 thr: 1.9 pred: 494.75\n",
      "---------- pred: 562.0\n",
      "---------- f: 1 thr: 11.82 pred: 472.333333333\n",
      "----------- pred: 495.0\n",
      "----------- f: 1 thr: 12.67 pred: 461.0\n",
      "------------ pred: 450.0\n",
      "------------ pred: 472.0\n",
      "--------- f: 1 thr: 11.65 pred: 605.666666667\n",
      "---------- pred: 562.0\n",
      "---------- f: 2 thr: 1.13 pred: 627.5\n",
      "----------- pred: 630.0\n",
      "----------- pred: 625.0\n",
      "-------- f: 9 thr: 1.15 pred: 624.769230769\n",
      "--------- f: 1 thr: 12.25 pred: 585.0\n",
      "---------- pred: 720.0\n",
      "---------- f: 3 thr: 2.4 pred: 562.5\n",
      "----------- f: 5 thr: 88.0 pred: 537.5\n",
      "------------ pred: 520.0\n",
      "------------ f: 1 thr: 12.81 pred: 555.0\n",
      "------------- pred: 560.0\n",
      "------------- pred: 550.0\n",
      "----------- f: 1 thr: 12.87 pred: 612.5\n",
      "------------ pred: 625.0\n",
      "------------ pred: 600.0\n",
      "--------- f: 5 thr: 80.0 pred: 671.166666667\n",
      "---------- pred: 580.0\n",
      "---------- f: 5 thr: 91.0 pred: 689.4\n",
      "----------- f: 0 thr: 2.0 pred: 670.666666667\n",
      "------------ f: 1 thr: 11.64 pred: 676.0\n",
      "------------- pred: 680.0\n",
      "------------- pred: 672.0\n",
      "------------ pred: 660.0\n",
      "----------- f: 0 thr: 2.0 pred: 717.5\n",
      "------------ pred: 710.0\n",
      "------------ pred: 725.0\n",
      "------ f: 7 thr: 2.45 pred: 504.0\n",
      "------- f: 6 thr: 2.2 pred: 483.0\n",
      "-------- f: 9 thr: 1.76 pred: 498.666666667\n",
      "--------- f: 3 thr: 2.32 pred: 486.285714286\n",
      "---------- f: 10 thr: 3.3 pred: 502.0\n",
      "----------- f: 1 thr: 12.16 pred: 491.5\n",
      "------------ pred: 495.0\n",
      "------------ pred: 488.0\n",
      "----------- f: 1 thr: 12.25 pred: 512.5\n",
      "------------ pred: 510.0\n",
      "------------ pred: 515.0\n",
      "---------- f: 2 thr: 1.51 pred: 465.333333333\n",
      "----------- pred: 450.0\n",
      "----------- f: 0 thr: 2.0 pred: 473.0\n",
      "------------ pred: 466.0\n",
      "------------ pred: 480.0\n",
      "--------- f: 1 thr: 11.84 pred: 542.0\n",
      "---------- pred: 520.0\n",
      "---------- pred: 564.0\n",
      "-------- pred: 342.0\n",
      "------- pred: 714.0\n",
      "---- f: 1 thr: 12.45 pred: 787.5\n",
      "----- pred: 880.0\n",
      "----- pred: 695.0\n",
      "--- f: 10 thr: 4.45 pred: 433.0\n",
      "---- f: 1 thr: 11.45 pred: 395.294117647\n",
      "----- f: 1 thr: 11.41 pred: 488.666666667\n",
      "------ f: 1 thr: 11.03 pred: 420.5\n",
      "------- pred: 407.0\n",
      "------- pred: 434.0\n",
      "------ pred: 625.0\n",
      "----- f: 6 thr: 2.45 pred: 375.285714286\n",
      "------ f: 1 thr: 12.22 pred: 301.0\n",
      "------- pred: 312.0\n",
      "------- pred: 290.0\n",
      "------ f: 1 thr: 12.37 pred: 387.666666667\n",
      "------- f: 1 thr: 12.08 pred: 399.444444444\n",
      "-------- f: 1 thr: 11.62 pred: 380.6\n",
      "--------- pred: 345.0\n",
      "--------- f: 1 thr: 11.82 pred: 389.5\n",
      "---------- pred: 415.0\n",
      "---------- f: 1 thr: 12.07 pred: 381.0\n",
      "----------- f: 1 thr: 11.87 pred: 379.0\n",
      "------------ pred: 380.0\n",
      "------------ pred: 378.0\n",
      "----------- pred: 385.0\n",
      "-------- f: 6 thr: 2.56 pred: 423.0\n",
      "--------- f: 1 thr: 12.29 pred: 433.0\n",
      "---------- pred: 428.0\n",
      "---------- pred: 438.0\n",
      "--------- f: 1 thr: 12.29 pred: 413.0\n",
      "---------- pred: 406.0\n",
      "---------- pred: 420.0\n",
      "------- f: 1 thr: 12.52 pred: 352.333333333\n",
      "-------- f: 1 thr: 12.43 pred: 338.5\n",
      "--------- pred: 352.0\n",
      "--------- pred: 325.0\n",
      "-------- pred: 380.0\n",
      "---- f: 6 thr: 2.6 pred: 646.666666667\n",
      "----- pred: 620.0\n",
      "----- pred: 660.0\n",
      "-- f: 6 thr: 3.18 pred: 669.923076923\n",
      "--- f: 7 thr: 1.09 pred: 647.791666667\n",
      "---- f: 9 thr: 1.11 pred: 741.25\n",
      "----- f: 7 thr: 0.6 pred: 617.5\n",
      "------ f: 1 thr: 12.84 pred: 580.0\n",
      "------- pred: 590.0\n",
      "------- pred: 570.0\n",
      "------ f: 0 thr: 2.0 pred: 655.0\n",
      "------- pred: 680.0\n",
      "------- pred: 630.0\n",
      "----- f: 1 thr: 13.27 pred: 803.125\n",
      "------ f: 1 thr: 12.29 pred: 846.0\n",
      "------- f: 0 thr: 2.0 pred: 862.5\n",
      "-------- pred: 870.0\n",
      "-------- pred: 855.0\n",
      "------- f: 1 thr: 13.16 pred: 835.0\n",
      "-------- pred: 830.0\n",
      "-------- f: 1 thr: 13.17 pred: 837.5\n",
      "--------- pred: 840.0\n",
      "--------- pred: 835.0\n",
      "------ f: 5 thr: 105.0 pred: 731.666666667\n",
      "------- pred: 750.0\n",
      "------- pred: 695.0\n",
      "---- f: 2 thr: 1.35 pred: 554.333333333\n",
      "----- f: 0 thr: 2.0 pred: 712.0\n",
      "------ f: 1 thr: 12.21 pred: 739.333333333\n",
      "------- pred: 718.0\n",
      "------- pred: 750.0\n",
      "------ pred: 630.0\n",
      "----- f: 2 thr: 1.9 pred: 475.5\n",
      "------ f: 0 thr: 2.0 pred: 375.0\n",
      "------- f: 1 thr: 12.17 pred: 350.0\n",
      "-------- pred: 355.0\n",
      "-------- pred: 345.0\n",
      "------- pred: 425.0\n",
      "------ f: 0 thr: 2.0 pred: 535.8\n",
      "------- f: 1 thr: 11.46 pred: 584.5\n",
      "-------- pred: 562.0\n",
      "-------- pred: 607.0\n",
      "------- f: 3 thr: 2.4 pred: 503.333333333\n",
      "-------- pred: 530.0\n",
      "-------- f: 1 thr: 12.79 pred: 490.0\n",
      "--------- pred: 480.0\n",
      "--------- pred: 500.0\n",
      "--- f: 1 thr: 11.96 pred: 935.5\n",
      "---- pred: 886.0\n",
      "---- pred: 985.0\n"
     ]
    }
   ],
   "source": [
    "# VARIANCE ON CONTINUOUS FEATURES\n",
    "\n",
    "dataset = wine\n",
    "\n",
    "print(\"sklearn DTR:\", eval(DTR, {}, dataset, mse)[\"score\"])\n",
    "evaluated = eval(DecisionTree, {\"max_depth\": None, \"min_size\": 2}, dataset, mse)\n",
    "print(\"DecisionTree:\", evaluated[\"score\"])\n",
    "print(\"Tree structure:\")\n",
    "print(evaluated[\"model\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
