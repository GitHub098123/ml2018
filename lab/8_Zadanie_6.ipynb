{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Zadanie 6\n",
    "\n",
    "Zaimplementować:\n",
    "\n",
    "klasę `TextMultinomialNaiveBayes`\n",
    "\n",
    "LUB\n",
    "\n",
    "klasę `TextBernoulliNaiveBayes`\n",
    "\n",
    "Będziemy używali tych klas tylko do przetwarzania tekstu, dlatego podana poniżej specyfikacja nieco różni się od zwykłej wersji modelu Naive Bayes. Proszę jednak pisać kod w taki sposób, żeby w razie potrzeby dało się łatwo usunąć logikę związaną z konwersją danych tekstowych - zadanie dodatkowe (niepunktowane) dla osób chętnych znajduje się w notebooku 8d.\n",
    "\n",
    "W każdym z powyższych przypadków długość implementacji nie powinna przekraczać 50 linijek kodu. Rozwiązania nieefektywne, nieczytelne, zawierające zagnieżdżone pętle itp. będą oceniane na zero punktów.\n",
    "\n",
    "Materiały uzupełniające wiedzę z wykładu:\n",
    "\n",
    "http://blog.datumbox.com/machine-learning-tutorial-the-naive-bayes-text-classifier/\n",
    "\n",
    "\n",
    "`__init__` przyjmuje dwa parametry:\n",
    "* `X` - lista lub tablica o rozmiarze (liczba_obserwacji,) typu str zawierająca zdania,\n",
    "* `y` - tablica o rozmiarze (liczba_obserwacji,) zawierająca numery klas (inty od 0 do n-1 w problemie n-klasowym)\n",
    "\n",
    "i uczy na tych danych model Naive Bayes.\n",
    "\n",
    "`X` należy w pierwszej kolejności przekształcić do dwuwymiarowej tablicy przy użyciu instancji klasy `CountVectorizer` (więcej na ten temat w notebooku 8a). Podobnie należy postąpić w przypadku liczenia predykcji. Proszę użyć `CountVectorizer(min_df=5)`, w ten sposób zredukujemy liczbę cech o około 80%.\n",
    "\n",
    "Klasa powinna mieć następujące atrybuty:\n",
    "\n",
    "`priors` - zwraca tablicę nauczonego rozkładu $p(\\mathrm{class})$ o rozmiarze (liczba_klas,).\n",
    "\n",
    "`likelihoods` - zwraca tablicę nauczonych prawdopodobieństw $p(\\mathrm{word}\\mid\\mathrm{class})$ (przy czym inaczej rozumiemy i obliczamy to prawdopodobieństwo w wypadku `TextMultinomialNaiveBayes`, a inaczej dla `TextBernoulliNaiveBayes`) o rozmiarze (liczba_klas, liczba_słów_w_zbiorze_treningowym).\n",
    "\n",
    "`features` - zwraca listę znalezionych słów (str) w kolejności zgodnej z drugim wymiarem `likelihoods` (patrz notebook 8a).\n",
    "\n",
    "Klasa implementuje następujące metody:\n",
    "\n",
    "`generate_sentences` - model Naive Bayes jest modelem __generatywnym__ - oznacza to, że podczas uczenia modelujemy nie tylko prawdopodobieństwo warunkowe $p(\\mathrm{class}\\mid\\mathrm{observation})$, ale __cały rozkład łączny__ $p(\\mathrm{class},\\mathrm{observation})$, a w takim razie możemy wykorzystać ten rozkład do samplowania nowych obserwacji; metoda przyjmuje numer klasy `nb_class` (w wersji `Multinomial` metoda przyjmuje też parametr `length` oznaczający długość generowanego zdania - dlaczego jest on niezbędny tylko w wersji `Multinomial`?) i generuje zdanie (string słów oddzielonych spacjami) z rozkładu warunkowego pod warunkiem tej klasy; oczywiście model Naive Bayes całkowicie ignoruje kolejność słów, więc ta metoda będzie generowała bezsensowne \"zdania\", nie mniej jednak wynik jest ciekawy.\n",
    "\n",
    "`predict_logits` - zwraca tabelę logitsów o rozmiarze (liczba_obserwacji, liczba_klas).\n",
    "\n",
    "https://stackoverflow.com/questions/41455101/what-is-the-meaning-of-the-word-logits-in-tensorflow\n",
    "\n",
    "Do obliczenia prawdopodobieństwa $p(\\mathrm{class}\\mid\\mathrm{observation})$ należy wykorzystać wzór Bayesa. W naszym wypadku licznik będzie iloczynem kilkuset (lub nawet kilkudziesięciu tysięcy) liczb. Takie obliczenie jest bardzo niestabilne numerycznie, dlatego w praktyce zastępuje się ten iloczyn logarytmem:\n",
    "$$\\ln(p(c)p(x_1\\mid c)\\cdots p(x_n\\mid c)) = \\ln p(c) + \\ln p(x_1\\mid c) + \\cdots + \\ln p(x_n\\mid c)$$\n",
    "który jest już znacznie przyjemniejszy do obliczenia. Niestety te logarytmy nie są znormalizowane - aby otrzymać logits należy jeszcze odjąć logarytm mianownika, ale obliczenie go znowu jest niestabilne numerycznie. Proszę zapoznać się z działaniem metody `unnormalized_logposteriors_to_logits` i zobaczyć, jak ona radzi sobie z tym problemem.\n",
    "\n",
    "`predict` - zwraca listę przewidzianych klas.\n",
    "\n",
    "### Uwaga dotycząca terminologii\n",
    "\n",
    "W wypadku modelu Naive Bayes __nauczone__ wartości $p(\\mathrm{class})$, $p(\\mathrm{observation}\\mid\\mathrm{class})$ oraz $p(\\mathrm{class}\\mid\\mathrm{observation})$ nazywane są odpowiednio prior, likelihood i posterior. Proszę pamiętać, że __to nie znaczy, że model był uczony Bayesowsko__.\n",
    "\n",
    "W jaki sposób należy zmodyfikować algorytm trenowania, aby uczenie również odbywało się Bayesowsko? Ile wzorów Bayesa trzeba będzie wtedy użyć?\n",
    "\n",
    "### Kiedy w praktyce należy używać modelu Naive Bayes\n",
    "\n",
    "Nigdy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
