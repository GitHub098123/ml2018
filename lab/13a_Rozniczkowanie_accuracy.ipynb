{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W całym niniejszym notebooku będziemy zakładać, że naszym modelem jest regresja logistyczna z parametrami $w,b$, a jej output oznacza prawdopodobieństwo klasy $y=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy - podejście klasyczne\n",
    "\n",
    "1. Oznaczmy przez $m_{w,b}$ wyjście naszego modelu. $m_{w,b}$ jest funkcją, która danej obserwacji $x\\in X$ przyporządkowuje rozkład prawdopodobieństwa na etykietach $y\\in Y$. \n",
    "\n",
    "2. Zadaniem tej funkcji jest modelować dla każdego $x$ prawdziwy rozkład warunkowy $p(y\\mid x)$\n",
    "$$\\forall_{x\\in X}: m_{w,b}(x) \\approx p(y\\mid x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Działanie\n",
    "\n",
    "1. Jeśli $Y$ jest zbiorem skończonym, to funkcja $m_{w,b}$ zwraca tyle liczb, ile jest elementów $Y$.\n",
    "\n",
    "2. Jeśli $Y$ ma dwa elementy możemy uprościć model i stwierdzić, że funkcja $m_{w,b}$ zwraca jedynie prawdopodobieństwo drugiego elementu, a prawdopodobieństwo pierwszego wyliczamy korzystając z faktu, że suma prawdopodobieństw wynosi $1$:\n",
    "\n",
    "  $$\\left\\{\\begin{array}{l}\\widehat p(y=1\\mid x) := m_{w,b}(x) \\\\ \n",
    "\\widehat p(y=0\\mid x) := 1 - m_{w,b}(x)\\end{array}\\right .$$\n",
    "\n",
    "3. Od tej pory $m_{w,b}(x)$ będzie w zależności od kontekstu oznaczało rozkład na $Y$ lub liczbowe prawdopodobieństwo klasy $y=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uczenie\n",
    "\n",
    "1. Uczenie polega na znalezieniu takich parametrów $w, b$, żeby powyższe przybliżenie było średnio (zgodnie z prawdopodobieństwem $p$) jak najlepsze. \n",
    "\n",
    "3. Uczenie gradientowe polega na wprowadzeniu różniczkowalnej funkcji kosztu, której argumentami są rozkłady $m_{w,b}(x)$ oraz $p(y\\mid x)$, a wartość jest tym mniejsza, im podobniejsze są do siebie te dwa rozkłady (wartość zero oznacza równość rozkładów). \n",
    "\n",
    "4. Teraz wystarczy zróżniczkować tę funkcję po parametrach $w,b$ i możemy minimalizować gradientowo koszt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy \n",
    "1. Załóżmy, że chcemy maksymalizować średnie accuracy, tzn. liczbę poprawnych predykcji etykiety $y$ na podstawie zaobserwowanego $x$. \n",
    "\n",
    "  * Od teraz będziemy nazywać tę predykcję __akcją__ (action). \n",
    "\n",
    "2. W ogólnym przypadku wybieranie akcji po zaobserwowaniu konkretnego $x$ nie musi być deterministyczne (tzw. strategie mieszane, mixed strategies)\n",
    "  * zdefiniujmy funkcję $\\pi$, która dla każdego $x$ zwraca rozkład prawdopodobieństwa na zbiorze $Y$ i rozkład ten oznacza tym razem, jak często wybieramy daną etykietę. \n",
    "  * W szczególności $\\pi(a\\mid s)$ oznacza wybór akcji (wyjscia, predykcję) $a$ w stanie $s$.\n",
    "\n",
    "3. Oczywiście $\\pi(x) \\neq p(y\\mid x)$ - jeśli np. dla danego $x$ etykieta $y=1$ ma prawdopodobieństwo $60\\%$, a etykieta $y=0$ tylko $40\\%$, to nigdy nie opłaca się przewidywać $y=0$. \n",
    "  * Wzór na optymalne $\\pi(x)$ jest następujący:\n",
    "\n",
    "  $$\\pi(x) = \\left\\{\\begin{array}{llll}\n",
    "    (y=0) \\mapsto 0; (y=1)\\mapsto 1 & \\mathrm{gdy} & p(y=1\\mid x) > 0,5 \\\\\n",
    "    (y=0) \\mapsto 1; (y=1)\\mapsto 0 & \\mathrm{gdy} & p(y=1\\mid x) < 0,5 \\\\\n",
    "    (y=0) \\mapsto q; (y=1)\\mapsto 1-q & \\mathrm{gdy} & p(y=1\\mid x) = 0,5 & q\\in[0,1]\\\\\n",
    "\\end{array}\\right .$$\n",
    "\n",
    "4. Czyli gdy $y=1$ ma prawdopodobieństwo większe niż $50\\%$, to zawsze podejmujemy akcję $y=1$. \n",
    "  * Jeśli to prawdopodobieństwo jest mniejsze od $50\\%$, to zawsze podejmujemy akcję $y=0$.   * Jeśli z kolei jest równe dokładnie $50\\%$, to możemy przyjąć dowolną strategię [dlaczego?]. \n",
    "  * Dla ułatwienia można wziąć np. $q=0$ i wtedy wzór przyjmuje postać:\n",
    "\n",
    "  $$\\pi(x) = \\left\\{\\begin{array}{llll}\n",
    "    (y=0) \\mapsto 0; (y=1)\\mapsto 1 & \\mathrm{gdy} & p(y=1\\mid x) \\geq 0,5 \\\\\n",
    "    (y=0) \\mapsto 1; (y=1)\\mapsto 0 & \\mathrm{gdy} & p(y=1\\mid x) < 0,5 \\\\\n",
    "\\end{array}\\right .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardowe podejście\n",
    "1. Standardowe podejście, które stosowaliśmy dotychczas, wygląda tak\n",
    "  * trenujemy funkcję $m_{w,b}(x)$, która przybliża $p(y\\mid x)$, \n",
    "  * i definiujemy:\n",
    "\n",
    "  $$\\pi_{w,b}(x) := \\pi_{m_{w,b}}(x) := \\left\\{\\begin{array}{llll}\n",
    "    (y=0) \\mapsto 0; (y=1)\\mapsto 1 & \\mathrm{gdy} & m_{w,b}(x) \\geq 0,5 \\\\\n",
    "    (y=0) \\mapsto 1; (y=1)\\mapsto 0 & \\mathrm{gdy} & m_{w,b}(x) < 0,5 \\\\\n",
    "\\end{array}\\right .$$\n",
    "\n",
    "2. $m_{w,b}(x)$ było do tej pory implementowane metodą `predict_proba`, a $\\pi_{m_{w,b}}(x)$ metodą `predict` \n",
    "  * (tu korzystaliśmy z faktu, że dla każdego $x$ predykcja jest faktycznie deterministyczna, więc nie zwracaliśmy rozkładu prawdopodobieństwa, tylko wartość tego $y$, który ma prawdopodobieństwo $100\\%$).\n",
    "\n",
    "3. Ale skoro interesuje nas __tylko accuracy__, to dlaczego nie uczymy modelu, który __od razu przewiduje akcję__?\n",
    "\n",
    "  * Czyli zamiast:\n",
    "  $$ m_{w,b}(x) \\sim p(y\\mid x);\\ \\pi_{w,b} := \\pi_{m_{w,b}} $$\n",
    "uczyć od razu model:\n",
    "$$ \\pi_{w,b}(x) \\sim \\pi(y\\mid x) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Nieróżniczkowalność\" accuracy\n",
    "\n",
    "1. Powszechnie mówi się, że accuracy jest nieróżniczkowalne - w takich sytuacjach chodzi o funkcję:\n",
    "\n",
    "  $$acc(y\\_true, y\\_pred) = \\dfrac{1}{N}\\sum_{j=1}^N (y\\_true_j == y\\_pred_j)$$\n",
    "\n",
    "2. Nieporozumienie polega na tym, że \n",
    "  * $y\\_pred$ jest rozumiane jako zestaw wylosowanych etykiet, \n",
    "  * __a nie prawdopodobieństwa__ $\\pi_{w,b}(x)$, z jakimi te etykiety były losowane. \n",
    "  \n",
    "3. Jeśli natomiast $y\\_pred$ będzie oznaczało prawdopodobieństwa, to możemy napisać [dlaczego to wciąż jest accuracy?]:\n",
    "\n",
    "  $$acc(y\\_true, y\\_pred) = \\dfrac{1}{N}\\sum_{j=1}^N y\\_true_j \\cdot y\\_pred_j + (1 - y\\_true_j) \\cdot (1 - y\\_pred_j)$$\n",
    "i ta funkcja jest już jak najbardziej różniczkowalna. \n",
    "\n",
    "4. Możemy więc zdefiniować loss jako:\n",
    "\n",
    "  $$\\begin{align}\n",
    "L(y\\_true, y\\_pred) &:= - acc(y\\_true, y\\_pred)\\\\\n",
    "&= -\\dfrac{1}{N}\\sum_{j=1}^N y\\_true_j \\cdot y\\_pred_j + (1 - y\\_true_j) \\cdot (1 - y\\_pred_j)\n",
    "\\end{align}$$\n",
    "\n",
    "5. Wzór ten jest bardzo podobny do kosztu binary cross-entropy - trzeba dobrze zrozumieć, czym się różnią, żeby nigdy ich nie pomylić! \n",
    "  * Przede wszystkim zwróćmy uwagę na postać optymalnego `y_pred`, które minimalizuje te koszty. \n",
    "  * Niech dla pewnego ustalonego $x$ mamy $p(y=1\\mid x) = 60\\%$, \n",
    "    * optymalizacja wzorem binary cross-entropy będzie dążyła do ustawienia predykcji dla tego $x$ na wartość $0.6$, \n",
    "    * podczas gdy optymalizacja accuracy będzie dążyła do wartośći $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dlaczego nie użyć wzoru na binary cross-entropy do optymalizacji accuracy?\n",
    "\n",
    "1. Powyżej musieliśmy \"ręcznie\" wprowadzić nowy wzór na różniczkowalne accuracy. \n",
    "  * Dlaczego nie da się użyć znanych wzorów do uczenia parametrów funkcji $\\pi_{w,b}$? \n",
    "  * Otóż żeby to zrobić, musielibyśmy wstawić do wzoru na binary cross-entropy funkcję $\\pi_{w,b}$ jako `y_pred` oraz rozkład $\\pi$ jako `y_true`. \n",
    "  * Problem polega na tym, że __nie mamy danych pochodzących z rozkładu $\\pi$__, ponieważ nasz dataset __pochodzi z rozkładu $p$__.\n",
    "\n",
    "2. Jak wyglądałby dataset pochodzący z rozkładu $\\pi$? \n",
    "  * Byłby to zestaw odpowiedzi \"wyroczni\" na pytanie \"dla danego przykładu $x$, która z etykiet ze zbioru $Y$ jest najbardziej prawdopodobna?\" \n",
    "  * czyli jeśli dla pewnego $x$ zachodzi $p(y=1\\mid x)=60\\%$, to wyrocznia zawsze odpowie \"$y=1$\", pomimo że w naszym oryginalnym datasecie jest spora szansa ($40$ procent), że etykieta będzie równa $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formalne wyprowadzenie wzoru na accuracy\n",
    "\n",
    "1. Accuracy to średnia liczba odgadnięć poprawnej etykiety. \n",
    "  * Oznaczmy literą $a \\in Y$ akcję, którą podejmujemy dla ustalonego $x\\in X$. \n",
    "    * odpowiada ona predykcji dla $x$\n",
    "  * Zbiór akcji oczywiście jest równy zbiorowi etykiet. \n",
    "  * Pary $x, y$ pochodzą z rozkładu $p$, podczas gdy akcja $a$ losowana jest z rozkładu $\\pi_{w,b}(x)$, gdzie $w,b$ to parametry, których się uczymy, a $x$ to obserwacja.\n",
    "\n",
    "2. Wprowadźmy jeszcze pojęcie __nagrody__ (reward), którą oznaczymy literą $r \\in \\mathbb{R}$. \n",
    "  * W wypadku accuracy nagroda wynosi $1$, gdy udało nam się poprawnie odgadnąć etykietę, i $0$ w przeciwnym przypadku:\n",
    "  $$r_{acc}(y,a) := \\left\\{\\begin{array}{lll}\n",
    "    1 & \\mathrm{gdy} & y = a \\\\\n",
    "    0 & \\mathrm{gdy} & y \\neq a \\\\\n",
    "\\end{array}\\right .$$\n",
    "\n",
    "3. Oznaczmy funkcją $\\upsilon(w,b)$ __średnią nagrodę__, którą uzyskamy stosując strategię $\\pi_{w,b}$ (czyli tutaj $\\upsilon(w,b)$ to po prostu accuracy modelu z parametrami $w, b$).\n",
    "\n",
    "4. Spróbujmy wyprowadzić krok po kroku wzór na średnią nagrodę:\n",
    "  $$\\upsilon(w,b) = \\ldots$$\n",
    "  * pierwszą średnią liczymy po wszystkich możliwych obserwacjach $x$ oraz prawdziwych etykietach $y$:\n",
    "  $$\\upsilon(w,b) = \\underset{x,y\\sim p}{\\mathbb{E}} \\ldots$$\n",
    "  * w tym momencie możemy zaobserwować __jedynie__ $x$ i na tej podstawie wylosować akcję $a$, którą podejmiemy:\n",
    "\n",
    "  $$\\upsilon(w,b) = \\underset{x,y\\sim p}{\\mathbb{E}}\\ \\underset{a\\sim\\pi_{w,b}(x)}{\\mathbb{E}}\\ldots$$\n",
    "\n",
    "  * na koniec musimy po prostu wstawić do wzoru wartość nagrody otrzymanej dla danych $x,y,a$:\n",
    "\n",
    "  $$\\upsilon(w,b) = \\underset{x,y\\sim p}{\\mathbb{E}}\\ \\underset{a\\sim\\pi_{w,b}(x)}{\\mathbb{E}}r_{acc}(y,a)$$\n",
    "\n",
    "  * Drugą wartość oczekiwaną możemy zamienić na całkę:\n",
    "  $$\\upsilon(w,b) = \\underset{x,y\\sim p}{\\mathbb{E}}\\ \\underset{Y}{\\int}\\pi_{w,b}(a\\mid x)\\cdot r_{acc}(y,a)\\ da$$\n",
    "     * $\\pi_{w,b}(a\\mid x)$ oznacza prawdopodobieństwo akcji $a$ w rozkładzie $\\pi_{w,b}(x)$\n",
    "\n",
    "  * zamieniając całki na sumy, podstawiając z definicji wartości $r_{acc}$ i grupując odpowiednio składniki, otrzymujemy wzór na accuracy z poprzedniej sekcji:\n",
    "\n",
    "  $$\\begin{align}\n",
    "  \\upsilon(w,b)&=\\underset{x,y\\sim p}{\\mathbb{E}}\\ \\underset{Y}{\\int}\\pi_{w,b}(a\\mid x)\\cdot r_{acc}(y,a)\\ da\\\\\n",
    "  &\\simeq \\dfrac{1}{n}\\sum_{j=1}^n y\\_true_j \\cdot y\\_pred_j + (1 - y\\_true_j) \\cdot (1 - y\\_pred_j)\n",
    "  \\end{align}$$\n",
    "\n",
    "#### Uwaga na zapis $\\pi_{w,b}(a\\mid x)$\n",
    "\n",
    "  Jeśli $\\pi_{w,b}(x)$ to output regresji logistycznej dla obserwacji $x$, to:\n",
    "\n",
    "  $$\\begin{align}\n",
    "\\pi_{w,b}(1\\mid x) &= \\pi_{w,b}(x) \\\\\n",
    "\\pi_{w,b}(0\\mid x) &= 1 - \\pi_{w,b}(x)\n",
    "\\end{align}$$\n",
    "i takie wartości należy wstawić do wzoru!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy gradient z epizodem o długości 1\n",
    "\n",
    "1. Wróćmy do wzoru:\n",
    "\n",
    "  $$\\upsilon(w,b) = \\underset{x,y\\sim p}{\\mathbb{E}}\\ \\underset{a\\sim\\pi_{w,b}(x)}{\\mathbb{E}}r_{acc}(y,a)$$\n",
    "\n",
    "  i spróbujmy go zróżniczkować po parametrach $w, b$ - w ten sposób wyprowadzimy __algorytm uczenia średniego accuracy__:\n",
    "\n",
    "  $$\\begin{align}\n",
    "\\dfrac{\\partial\\upsilon(w,b)}{\\partial w} &= \\dfrac{\\partial}{\\partial w}\\underset{x,y\\sim p}{\\mathbb{E}}\\ \\underset{a\\sim\\pi_{w,b}(x)}{\\mathbb{E}}r_{acc}(y,a) \\\\\n",
    "&=\\underset{x,y\\sim p}{\\mathbb{E}}\\ \\dfrac{\\partial}{\\partial w}\\underset{a\\sim\\pi_{w,b}(x)}{\\mathbb{E}}r_{acc}(y,a)\\\\\n",
    "&=\\underset{x,y\\sim p}{\\mathbb{E}}\\ \\dfrac{\\partial}{\\partial w} \\underset{Y}{\\int}\\pi_{w,b}(a\\mid x)\\cdot r_{acc}(y,a)\\ da\\\\\n",
    "&=\\underset{x,y\\sim p}{\\mathbb{E}}\\ \\underset{Y}{\\int}\\dfrac{\\partial \\pi_{w,b}(a\\mid x)}{\\partial w}\\cdot r_{acc}(y,a)\\ da = \\ldots\n",
    "\\end{align}$$\n",
    "\n",
    "2. Umiemy policzyć $\\dfrac{\\partial \\pi_{w,b}(x)(a)}{\\partial w}$, bo znamy wzór na nasz model. \n",
    "  * Wykonajmy jeszcze jeden trick (odwrotny do tego z poprzedniej ramki), który sprawi, że całka zamieni się z powrotem na wartość oczekiwaną:\n",
    "\n",
    "  $$\\begin{align}\n",
    "\\ldots&=\\underset{x,y\\sim p}{\\mathbb{E}}\\ \\underset{Y}{\\int}\\pi_{w,b}(a\\mid x)\\dfrac{1}{\\pi_{w,b}(a\\mid x)} \\dfrac{\\partial \\pi_{w,b}(a\\mid x)}{\\partial w}\\cdot r_{acc}(y,a)\\ da\\\\\n",
    "&=\\underset{x,y\\sim p}{\\mathbb{E}}\\ \\underset{a\\sim\\pi_{w,b}(x)}{\\mathbb{E}}\\dfrac{1}{\\pi_{w,b}(a\\mid x)} \\dfrac{\\partial \\pi_{w,b}(a\\mid x)}{\\partial w}\\cdot r_{acc}(y,a) = \\ldots\n",
    "\\end{align}$$\n",
    "\n",
    "3. Ostatnia modyfikacja - wyłącznie w celu skrócenia zapisu (reguła łańcuchowa):\n",
    "\n",
    "  $$\\ldots = \\underset{x,y\\sim p}{\\mathbb{E}}\\ \\underset{a\\sim\\pi_{w,b}(x)}{\\mathbb{E}}\\dfrac{\\partial(\\ln\\pi_{w,b}(a\\mid x))}{\\partial w}\\cdot r_{acc}(y,a)$$\n",
    "\n",
    "4. Ostatecznie:\n",
    "  $$\\dfrac{\\partial\\upsilon(w,b)}{\\partial w} = \\underset{x,y\\sim p}{\\mathbb{E}}\\ \\underset{a\\sim\\pi_{w,b}(x)}{\\mathbb{E}}\\dfrac{\\partial(\\ln\\pi_{w,b}(a\\mid x))}{\\partial w}\\cdot r_{acc}(y,a)$$\n",
    "\n",
    "5. i analogicznie:\n",
    "  $$\\dfrac{\\partial\\upsilon(w,b)}{\\partial b} = \\underset{x,y\\sim p}{\\mathbb{E}}\\ \\underset{a\\sim\\pi_{w,b}(x)}{\\mathbb{E}}\\dfrac{\\partial(\\ln\\pi_{w,b}(a\\mid x))}{\\partial b}\\cdot r_{acc}(y,a)$$\n",
    "\n",
    "6. W związku z tym pochodną $\\dfrac{\\partial\\upsilon(w,b)}{\\partial w}$ (analogicznie pochodna po $b$) można aproksymować w następujący sposób:\n",
    "  1. Wylosuj dane $x,y$ z prawdziwego rozkładu (lub weź ze zbioru treningowego).\n",
    "  2. Wylosuj akcję $a$ z rozkładu $\\pi_{w,b}(x)$.\n",
    "  3. Policz pochodną $\\dfrac{\\partial(\\ln\\pi_{w,b}(a\\mid x))}{\\partial w}$ i przemnóż ją przez nagrodę $r_{acc}(y,a)$.\n",
    "  4. Powtórz kroki 1-3 wielokrotnie, wyniki uśrednij."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uogólnienie na dowolną nagrodę\n",
    "\n",
    "1. Zauważmy, że wyprowadzając wzór na gradient średniej nagrody nie musieliśmy rozumieć, jak działa $r_{acc}$ - wystarczy jedynie znać jej wartość. \n",
    "  * Okazuje się, że $r$ może być praktycznie dowolną funkcją - nie musi być deterministyczna, różniczkowalna, a nawet ciągła!\n",
    "\n",
    "2. __Policy gradient__ mówi o tym, jak poprawić parametry naszej strategii, żeby zwiększyć średnią nagrodę. \n",
    "  * Wprowadza się dwa pojęcia: agent i środowisko. \n",
    "  * Strategia jest częścią agenta, który wykonuje akcje i uczy się maksymalizować średnią nagrodę, \n",
    "  * środowisko dostarcza obserwacji i liczbowych wartości nagrody.\n",
    "\n",
    "3. W tym ogólnym przypadku wzory na gradient wyglądają następująco:\n",
    "  $$\\begin{align}\n",
    "\\dfrac{\\partial\\upsilon(w,b)}{\\partial w} &= \\underset{x\\sim p}{\\mathbb{E}}\\ \\underset{a\\sim\\pi_{w,b}(x)}{\\mathbb{E}}\\dfrac{\\partial(\\ln\\pi_{w,b}(a\\mid x))}{\\partial w}\\cdot r(a) \\\\\n",
    "\\dfrac{\\partial\\upsilon(w,b)}{\\partial b} &= \\underset{x\\sim p}{\\mathbb{E}}\\ \\underset{a\\sim\\pi_{w,b}(x)}{\\mathbb{E}}\\dfrac{\\partial(\\ln\\pi_{w,b}(a\\mid x))}{\\partial b}\\cdot r(a)\n",
    "\\end{align}$$\n",
    "\n",
    "4. uczenie agenta przebiega w pętli:\n",
    "  1. Środowisko losuje obserwację $x$ z nieznanego rozkładu $p$ i przekazuje ją agentowi.\n",
    "  2. Agent losuje akcję $a$ z rozkładu $\\pi_{w,b}(x)$ i przekazuje ją do środowiska.\n",
    "  3. Środowisko zwraca agentowi nagrodę $r(a)$.\n",
    "  4. Agent oblicza pochodną $\\dfrac{\\partial(\\ln\\pi_{w,b}(a\\mid x))}{\\partial w}$ i mnoży ją przez nagrodę $r(a)$.\n",
    "  5. Agent powtarza kroki 1-4 wielokrotnie, a następnie uśrednia wyniki z kroku 4, mnoży je przez learning rate i dokonuje pojedynczego update'u swoich parametrów.\n",
    "  6. Kroki 1-5 powtarzamy do czasu, aż agent się nauczy.\n",
    "\n",
    "5. Proszę pamiętać, że w kroku 5 __nie dopisujemy znaku minus przy gradiencie__ - znak minus pojawia się przy __minimalizowaniu__ funkcji kosztu, natomiast tutaj chcemy __maksymalizować__ średnią nagrodę.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co oznacza \"epizod o długości 1\"?\n",
    "\n",
    "1. Nagroda wypłacana przez środowisko może zależeć nie tylko od akcji, którą wykonaliśmy w danym kroku, ale też od całej historii interakcji agenta ze środowiskiem. Długość epizodu to liczba kolejnych wypłacanych nagród, które są od siebie zależne.\n",
    "\n",
    "2. W powyższych wzorach zakładaliśmy, że wszystkie akcje są niezależne - losowaliśmy obserwację $x$ z ustalonego (ale nieznanego) rozkładu $p$, wykonywaliśmy akcję $a$, a nagroda była obliczana tylko na podstawie tych dwóch wielkości. Kolejna iteracja pętli przebiegała całkowicie niezależnie od poprzedniej.\n",
    "\n",
    "3. Można rozumieć to tak, że epizod o długości 1 oznacza, iż obserwacje są I.I.D., a środowisko nie ma żadnej \"pamięci\", w której mogłoby przechowywać akcje wykonane przez agenta (lub środowisko jest bezstanowe, czyli agent nie może go zmodyfikować). \n",
    "  * To założenie sprawdza się w problemie klasyfikacji, ale w praktyce oczywiście taka sytuacja nie występuje nigdy - na następnych ćwiczeniach wprowadzimy agenta, który uczy się z dłuższych epizodów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Odpowiedź na pytanie\n",
    "\n",
    "1. Gdyby we wzorze na policy gradient zamiast drugiej wartości oczekiwanej wstawić całkę, to zamiast samplować jedną akcję $a$ musielibyśmy policzyć sumę po wszystkich możliwych akcjach. \n",
    "2. W wypadku maksymalizowania accuracy da się to oczywiście zrobić (i tak będzie lepiej, bo zamiast aproksymacji mamy wartość dokładną), ale w ogólnym przypadku środowisko nie pozwoli nam wykonać więcej niż jednej akcji dla danej obserwacji $x$, czyli nie mamy możliwości poznania wartości reward dla wszystkich akcji. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
